[{"body":"Krustlet documentation Everything you need to know about Krustlet.\nHow the documentation is organized Krustlet has a lot of documentation. A high-level overview of how it’s organized will help you know where to look for certain things:\n The introduction takes you by the hand through a series of steps to run an application on Krustlet. Start here if you’re new to Krustlet. Topic guides discuss key topics and concepts at a fairly high level and provide useful background information and explanation. Community Guides teach you about the development process for Krustlet and how you can contribute to the project. How To Guides are recipes. They guide you through the steps involved in addressing key problems and use-cases. They are more advanced than tutorials and assume some knowledge of how Krustlet works.  ","excerpt":"Krustlet documentation Everything you need to know about Krustlet.\nHow …","ref":"/","title":""},{"body":"Community guides Interested in contributing to Krustlet? Start here:\n Code of Conduct Developer guide Release Checklist  ","excerpt":"Community guides Interested in contributing to Krustlet? Start here: …","ref":"/community/","title":""},{"body":"Code of Conduct This project has adopted the Microsoft Open Source Code of Conduct.\nFor more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\n","excerpt":"Code of Conduct This project has adopted the Microsoft Open Source …","ref":"/community/code-of-conduct/","title":""},{"body":"Developer guide This guide explains how to set up your environment for developing Krustlet.\nPrerequisites To build krustlet, you will need\n The latest stable version of Rust The latest version of just openssl (Or use the rustls-tls feature) git  If you want to test krustlet, you will also require\n A Kubernetes cluster The latest version of kubectl  If you want to compile your own WebAssembly modules and upload them to a registry, you’ll need wasm-to-oci.\nIf you want to build the Docker image, you’ll need Docker.\nBuilding We use just to build our programs, but you can use cargo if you want:\n$ just build Building without openssl If you are on a system that doesn’t have OpenSSL (or has the incorrect version), you have the option to build Krustlet using the Rustls project (Rust native TLS implementation):\n$ just build --no-default-features --features rustls-tls The same flags can be passed to just run if you want to just run the project instead.\nCaveats The underlying dependencies for Rustls do not support certs with IP SANs (subject alternate names). Because of this, the serving certs requested during bootstrap will not work for local development options like minikube or KinD as they do not have an FQDN\nBuilding on WSL (Windows Subsystem for Linux) You can build Krustlet on WSL but will need a few prerequisites that aren’t included in the Ubuntu distro in the Microsoft Store:\n$ sudo apt install build-essential libssl-dev pkg-config NOTE: We’ve had mixed success developing Krustlet on WSL. It has been successfully run on WSL2 using the WSL2-enabled Docker Kubernetes or Azure Kubernetes. If you’re on WSL1 you may be better off running in a full Linux VM under Hyper-V.\nBuilding on Windows As of version 0.4, we have support for building on Windows. For convenience sake, there is a windows version of the justfile called justfile-windows. This justfile uses PowerShell and has the proper flags set for Windows builds. To use it, you’ll have to specify the justfile using the --justfile flag like so:\n$ just --justfile justfile-windows build It has all the same targets as the normal justfile, however, the test target runs a little differently than the normal target due to how we use feature flags. This means there will be some spurious warning output from clippy, but the tests will run.\nNOTE: Windows builds use the rustls library, which means there are some things to be aware of. See the caveats section for more details\nRunning The default included runtime with Krustlet is wasi.\nThe wasi runtime uses a project called wasmtime. wasmtime is a standalone JIT-style host runtime for WebAssembly modules. It is focused primarily on standards compliance with the WASM specification as it relates to WASI. If your WebAssembly module complies with the WebAssembly specification, wasmtime can run it.\nBefore startup, this command will delete any nodes in your Kubernetes cluster named with your hostname, so make sure you’re running this in a test environment.\nIf you want to interact with the kubelet (for things like kubectl logs and kubectl exec), you’ll likely need to set a specific KRUSTLET_NODE_IP that krustlet will be available at. Otherwise, calls to the kubelet will result in errors. This may differ from machine to machine. For example, with Minikube on a Mac, you’ll have an interface called bridge0 which the cluster can talk to. So your node IP should be that IP address.\nTo set the node IP, run:\n$ export KRUSTLET_NODE_IP=\u003cthe ip address\u003e Testing Krustlet contains both integration and unit tests. For convenience, there are just targets for running one or the other.\nFor unit tests:\n$ just test For the integration tests, start a WASI node in a separate terminal before running the tests.\nIn terminal 1:\n$ just run-wasi And in terminal 2:\n$ just test-e2e You can run the integration tests without creating additional terminals or manually running the kubelets by running:\n$ just test-e2e-standalone This:\n Bootstraps and approves certificates if necessary Runs the WASI kubelet in the background Runs the integration tests Terminates the kubelets when the integration tests complete Reports test failures, and saves the kubelet logs if any tests failed  You will still need to set KRUSTLET_NODE_IP because the tester doesn’t know what kind of Kubernetes cluster you’re using and so doesn’t know how to infer a node IP.\nWARNING: The standalone integration tester has not been, er, tested on Windows. Hashtag irony.\nIntegration test debris There are some failure modes - for example image pull timeout - where the integration tests are not able to complete cleanup of their resources. Specifically you can sometimes get pods stuck in Terminating, which prevents namespace cleanup and causes the next test run to break.\nYou can forcibly clean up such debris by running cargo run --bin podsmiter. You may need to wait a couple of minutes after pod deletion for the namespaces to be collected.\nCreating your own Kubelets with Krustlet If you want to create your own Kubelet based on Krustlet, all you need to do is implement a Provider.\nSee src/krustlet-wasi.rs and its corresponding provider implementation in crates/wasi-provider to get started.\n","excerpt":"Developer guide This guide explains how to set up your environment for …","ref":"/community/developers/","title":""},{"body":"Release Checklist  If your experience deviates from this document, please document the changes to keep it up-to-date.\n This guide will guide you (redundancy added for purposes of punnery) on your journey to release a new version of Krustlet.\nAll releases will be of the form vX.Y.Z where X is the major version number, Y is the minor version number and Z is the patch release number. This project strictly follows semantic versioning so following this step is critical.\nIt is important to note that this document assumes that the git remote in your repository that corresponds to https://github.com/deislabs/krustlet is named upstream. If yours is not (for example, if you’ve chosen to name it origin or something similar instead), be sure to adjust the listed snippets for your local environment accordingly.\nIf you are not sure what your upstream remote is named, use a command like git remote -v to find out.\nIf you don’t have an upstream remote, you can add one easily using the following command:\ngit remote add upstream git@github.com:deislabs/krustlet We are also going to be adding security and verification of the release process by providing signature files. We perform this using GitHub and GPG.\nIf you do not have GPG already setup you can follow these steps:\n Install the GNU privacy Guard (GPG) Generate a new GPG key Add your key to your GitHub account Set your signing key in Git  Step 0: determine what type of release you are cutting Major releases are for new feature additions and behavioral changes that break backwards compatibility. Minor releases are for new feature additions that do not break backwards compatibility. Patch releases are for bug fixes that do not introduce any new features.\nThere are two versions of the checklist: one for patch releases, and one for major/minor releases. Follow the checklist that best applies below.\nChecklist for major and minor releases Follow this checklist if you are cutting a major release (vX.0.0) or a minor release (vX.Y.0):\n set up your environment increment the version number finalize the release and write the release notes  In this checklist, we are going to reference a few environment variables which you will want to set for convenience.\nFor major and minor releases, set the following environment variables, changing the values of $MAJOR_RELEASE_NUMBER, $MINOR_RELEASE_NUMBER, and $RELEASE_CANDIDATE_NUMBER accordingly:\nexport MAJOR_RELEASE_NUMBER=\"0\" export MINOR_RELEASE_NUMBER=\"1\" export RELEASE_NAME=\"v${MAJOR_RELEASE_NUMBER}.${MINOR_RELEASE_NUMBER}.0\" Now that we know we are going to cut a major or a minor release, we will need to bump the project’s Cargo.toml as well as any crates that have been updated this release.\nOpen a new pull request against krustlet, bumping the version fields in Cargo.toml:\n https://github.com/deislabs/krustlet/blob/master/Cargo.toml#L3  If applicable, in that same pull request, bump the version fields for the kubelet and oci-distribution crates:\n https://github.com/deislabs/krustlet/blob/master/crates/kubelet/Cargo.toml#L3 https://github.com/deislabs/krustlet/blob/master/crates/oci-distribution/Cargo.toml#L3  Use the following commands to create the commit:\ngit add . git commit --gpg-sign -m \"bump version to $RELEASE_NAME\" Wait until the pull request has been merged into master before proceeding.\nAfter the pull request has been merged, create a new tag from upstream’s master branch.\ngit fetch upstream master git checkout upstream/master git tag --sign --annotate \"${RELEASE_NAME}\" --message \"Krustlet release ${RELEASE_NAME}\" Double-check one last time to make sure everything is in order, then finally push the release tag.\ngit push upstream $RELEASE_NAME It is usually more beneficial to the end-user if the release notes are hand-written by a human being/marketing team/dog, so we’ll go ahead and write up the release notes.\nIf you’re releasing a major/minor release, listing notable user-facing features is usually sufficient. For patch releases, do the same, but make note of the symptoms causing the original issue, who may be affected, and how the patch mitigates the issue.\nit should look like this:\n## Krustlet vX.Y.Z  Krustlet vX.Y.Z is a feature release. This release, we focused on \u003cinsert focal point here\u003e. Users are encouraged to upgrade for the best experience. ## Notable features/changes  - TLS bootstrapping support has been added - Improved error handling for the Kubelet API ## Breaking changes  No known breaking changes were introduced this release. ## Known issues/missing features  - Needs more cowbell - I gotta have more cowbell! ## Installation  Download Krustlet vX.Y.Z: - [checksums-vX.Y.Z.txt](https://krustlet.blob.core.windows.net/releases/checksums-vX.Y.Z.txt) - [krustlet-vX.Y.Z-linux-amd64.tar.gz](https://krustlet.blob.core.windows.net/releases/krustlet-vX.Y.Z-linux-amd64.tar.gz) - [krustlet-vX.Y.Z-macos-amd64.tar.gz](https://krustlet.blob.core.windows.net/releases/krustlet-vX.Y.Z-macos-amd64.tar.gz) Feel free to bring in your own personality into the release notes; it’s nice for people to think we’re not all robots. :)\nDouble check the URLs are correct. Once finished, go into GitHub and edit the release notes for the tagged release with the notes written here.\nIt is now worth getting other people to take a look at the release notes before the release is published. It is always beneficial as it can be easy to miss something.\nFor pre-v1.0 releases, make sure to check the checkbox that says “This is a pre-release” to notify users that this release is identified as non-production ready.\nWhen you are ready to go, hit publish, and you’re done.\nChecklist for patch releases Follow this checklist if you are cutting a patch release (vX.Y.Z). The process is largely the same as cutting a major/minor release, but with a few differences.\n set up your environment increment the version number cherry-pick fixes finalize the release and write the release notes  In this checklist, we are going to reference a few environment variables which you will want to set for convenience.\nFor patch releases, set the following environment variables:\nexport MAJOR_RELEASE_NUMBER=\"0\" export MINOR_RELEASE_NUMBER=\"1\" export PATCH_RELEASE_NUMBER=\"1\" export PREVIOUS_PATCH_RELEASE_NUMBER=\"0\" export RELEASE_NAME=\"v${MAJOR_RELEASE_NUMBER}.${MINOR_RELEASE_NUMBER}.${PATCH_RELEASE_NUMBER}\" export PREVIOUS_RELEASE_NAME=\"v${MAJOR_RELEASE_NUMBER}.${MINOR_RELEASE_NUMBER}.${PREVIOUS_PATCH_RELEASE_NUMBER}\" export RELEASE_BRANCH_NAME=\"release-${MAJOR_RELEASE_NUMBER}.${MINOR_RELEASE_NUMBER}.${PATCH_RELEASE_NUMBER}\" Now that we know we are going to cut a patch release, we will need to bump the project’s Cargo.toml as well as any crates that have been updated this release.\nOpen a new pull request against krustlet, bumping the version fields in Cargo.toml:\n https://github.com/deislabs/krustlet/blob/master/Cargo.toml#L3  If applicable, in that same pull request, bump the version fields for the kubelet and oci-distribution crates:\n https://github.com/deislabs/krustlet/blob/master/crates/kubelet/Cargo.toml#L3 https://github.com/deislabs/krustlet/blob/master/crates/oci-distribution/Cargo.toml#L3  Use the following commands to create the commit:\ngit add . git commit --gpg-sign -m \"bump version to $RELEASE_NAME\" Wait until the pull request has been merged into master before proceeding.\nAfter the pull request has been merged, checkout the previous tag from upstream and use it to create a new branch:\ngit fetch upstream --tags git checkout $PREVIOUS_RELEASE_NAME git checkout -b $RELEASE_BRANCH_NAME Once that’s done, make sure to cherry-pick the fixes as well as the version bump from the previous step into this branch:\ngit cherry-pick -x \u003ccommit_id\u003e If anyone is available, let others peer-review the branch before continuing to ensure that all the proper fixes have been merged into the release branch.\nWhen you’re finally happy with the quality of the release branch, you can move on and create the tag. Double-check one last time to make sure everything is in order, then finally push the release tag.\ngit tag --sign --annotate \"${RELEASE_NAME}\" --message \"Krustlet release ${RELEASE_NAME}\" git push upstream $RELEASE_NAME It is usually more beneficial to the end-user if the release notes are hand-written by a human being/marketing team/dog, so we’ll go ahead and write up the release notes.\nFor patch releases, make note of the symptoms causing the original issue(s), who may be affected, and how the patch(es) mitigate the issue(s).\nit should look like this:\n## Krustlet vX.Y.Z  Krustlet vX.Y.Z is a patch release, fixing an issue where \u003cinsert disagnosis of issue here\u003e. This patch fixes this issue by \u003cinsert remedy here\u003e. Users are encouraged to upgrade for the best experience. ## Breaking changes  No known breaking changes were introduced this release. ## Installation  Download Krustlet vX.Y.Z: - [checksums-vX.Y.Z.txt](https://krustlet.blob.core.windows.net/releases/checksums-vX.Y.Z.txt) - [krustlet-vX.Y.Z-linux-amd64.tar.gz](https://krustlet.blob.core.windows.net/releases/krustlet-vX.Y.Z-linux-amd64.tar.gz) - [krustlet-vX.Y.Z-macos-amd64.tar.gz](https://krustlet.blob.core.windows.net/releases/krustlet-vX.Y.Z-macos-amd64.tar.gz) Double check the URLs are correct. Once finished, go into GitHub and edit the release notes for the tagged release with the notes written here.\nIt is now worth getting other people to take a look at the release notes before the release is published. It is always beneficial as it can be easy to miss something.\nFor pre-v1.0 releases, make sure to check the checkbox that says “This is a pre-release” to notify users that this release is identified as non-production ready.\nWhen you are ready to go, hit publish, and you’re done.\n","excerpt":"Release Checklist  If your experience deviates from this document, …","ref":"/community/release-checklist/","title":""},{"body":"How-To Guides Here you’ll find short answers to “How do I…?” types of questions. These how-to guides don’t cover topics in depth – you’ll find that material in the topic guides section. However, these guides will help you quickly accomplish common tasks.\n  Running Krustlet on Azure\n  Running Krustlet on Amazon Elastic Kubernetes Service (EKS)\n  Running Krustlet on Google Kubernetes Engine (GKE)\n  Running Krustlet on Kubernetes-in-Docker (KinD)\n  Running Krustlet on Minikube\n  Running Krustlet on any Kubernetes cluster with inlets\n  Running Krustlet on MicroK8s\n  Running Kubernetes on Amazon Elastic Kubernetes Service (EKS)\n  Running Kubernetes on Kubernetes-in-Docker (KinD)\n  Running Kubernetes on Minikube\n  Running Web Assembly (WASM) workloads in Kubernetes\n  Registering a CSI driver\n  ","excerpt":"How-To Guides Here you’ll find short answers to “How do I…?” types of …","ref":"/howto/","title":""},{"body":"Bootstrapping Krustlet As of version 0.3.0, Krustlet supports automatic bootstrapping of its authorization and serving certificates. This document describes how the functionality works.\nInitialization Krustlet follows the same initialization flow as Kubelet (with the exception of automatic renewal of certs that are close to expiry).\nInstructions In order to join a cluster with the proper permissions, Krustlet requires a valid bootstrap config with a valid bootstrap token. This token can be generated with kubeadm or may already exist depending on your provider. However, in this case, we will be using an easier method for creating a join token. Either way, the examples here should be useful for figuring out how to do it differently depending on your setup.\nPrerequisites You will need kubectl installed and a kubeconfig that has access to create Secrets in the kube-system namespace and can approve CertificateSigningRequests.\nGenerating a token and kubeconfig We have a useful bootstrapping bash script or Powershell script that can be used for generating a token and creating a bootstrap kubeconfig file. If you have cloned the repo, you can run:\n$ ./docs/howto/assets/bootstrap.sh OR\n$ .\\docs\\howto\\assets\\bootstrap.ps1 If you are the trusting sort, you can pipe it in from the internet:\n$ bash \u003c(curl https://raw.githubusercontent.com/deislabs/krustlet/master/docs/howto/assets/bootstrap.sh) OR\n(Invoke-WebRequest -UseBasicParsing https://raw.githubusercontent.com/deislabs/krustlet/master/docs/howto/assets/bootstrap.ps1).Content | Invoke-Expression This will output a ready-to-use bootstrap config to $HOME/.krustlet/config/bootstrap.conf\nScript configuration The script also exposes a few configuration options by means of environment variables. These are detailed in the table below:\n   Name Description Default     CONFIG_DIR The location of your configuration directory for Krustlet. Should be the same as $KRUSTLET_DATA_DIR/config where the KRUSTLET_DATA_DIR setting is the same one you use for configuring Krustlet $HOME/.krustlet/config   FILE_NAME The name of the file the bootstrap config should be saved to bootstrap.conf    Nitty-gritty details This section contains an overview of the nitty-gritty details for those who may be constructing their own bootstrapping setup. Feel free to skip this section if it doesn’t pertain to you.\nBootstrap tokens A bootstrap token has the format of [a-z0-9]{6}.[a-z0-9]{16} where the first part is a randomly generated token id and the second part after the . needs to be a cryptographically secure random string. The token will look something like this: ke3uxh.vhxb3ttj1nquno5t. That means you can generate a token with a simple bash command like so:\n$ echo \"$(\u003c /dev/urandom tr -dc a-z0-9 | head -c${1:-6};echo;).$(\u003c /dev/urandom tr -dc a-z0-9 | head -c${1:-16};echo;)\" Creating the secret To actually “create” the bootstrap token, it needs to be placed in a Secret in the kube-system namespace. The name of the secret should be bootstrap-token-\u003ctoken_id\u003e. Specifically, the secret should look something like this when you send it to the API:\napiVersion:v1kind:Secretmetadata:name:bootstrap-token-\u003ctoken_id\u003enamespace:kube-systemtype:bootstrap.kubernetes.io/tokenstringData:auth-extra-groups:system:bootstrappers:kubeadm:default-node-tokenexpiration:2020-06-04T20:07:24Ztoken-id:\u003ctoken_id\u003etoken-secret:\u003ctoken_secret\u003eusage-bootstrap-authentication:\"true\"usage-bootstrap-signing:\"true\"The main fields you need to set are token-id, token-secret, and expiration.\nGenerating the Kubeconfig Once you have a Secret created, you then have to generate the Kubeconfig. To do so, you’ll need several pieces of information:\n The CA cert for your Kubernetes API. This should be available from the kubeconfig you are currently using The server hostname or IP address The generated bootstrap token  You can either assemble a kubeconfig by hand or use similar steps to what is found in the bootstrap script\nAn example bootstrap config This is an example of a bootstrap config file for reference if creating your own workflow\napiVersion:v1clusters:- cluster:certificate-authority-data:LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwdGFXNXAKYTNWaVpVTkJNQjRYRFRFNE1EUXlOREF3TVRVME9Wb1hEVEk0TURReU1UQXdNVFUwT1Zvd0ZURVRNQkVHQTFVRQpBeE1LYldsdWFXdDFZbVZEUVRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS1lxCnhzMllBNzRETlFQTVZkbFJ3aXZFWnIwd0lTUHlQZjkzR3ZsUVNKMDQrbFgxSEF3Yi9GM3dqcDVEckVOSDAraTUKbjhZUy9QK3JlNUpqVU9tV1VmMXNtMmVLNHNRNHpNS01kMHc5by9ERlozTHc3K1h6RzMveTdvMkF4SWVlYjBPdgpzbzhwWUVOMklzRUcrRFhpa0l0MjhPZ1RtZGRhTVg1OWJQTXhGL0l6T1FPVmFEYmtnMk5ScWtjYW9CR0FTT2JkCkVId2hrVGdMYXZCNzVnVmRTVVlWUFU1M2dXc3hDQWVBYzNCaW9NekNLNmFFUXIrMDB4V3dEWkR4amxLYU02V3gKTWFQN0JmY0Y5K2U3OUt0Tkc3TXZMWG9xdFJ3cCtPdWREaTlKWHRLS1NNbVo3TFNubEY4UDdHUlhKL2IzNS9NVQpvQklkK1ZKNHpoak5zT2xKM3g4Q0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUIwR0ExVWRKUVFXCk1CUUdDQ3NHQVFVRkJ3TUNCZ2dyQmdFRkJRY0RBVEFQQmdOVkhSTUJBZjhFQlRBREFRSC9NQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFDWmM0SVpST3pnNS91eDdNa0Y3NmVja3dZekY4OUJiejRhRENVS3ByWUMxTDFvZVBVawpXdFc3VENWditDMDJRc2tGRnpTbGhlQUpYeXp5Q2xKMVE5VmUyUmR2bGtiZHVPTXpJeXZTS0xaZHVDT2pvVWNZCjkrMTN1UEFpaXJjNUpRZlBOTGdJcUdhbTB2ZXpqZEtROHNUK0o0WmRyNHdFOUZKQnhOeUhob0xQdjBLRENBbkcKWFlPZW5lTHdjcnJCcTVDTERRN2dIelZGbEFKVU1nSWF3ZzdtcG1HVi9KRlVlYnRpam1Cd1p1WDBNMTFpVHBqYQpNaUhDRkJOREd5a2locDBoSHdDV1ZId0ZXNHVLUWxUZjRBK2hieC9OTUkzbHhBYXozMFZKN3U1Mm1GR3pCQ0dvCmt3VjdKS2RJMEd1MGJQQmlUSDRMTmE4bWxqYmZkRnhsc2k4cAotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==server:https://192.168.64.19:8443name:minikubecontexts:- context:cluster:minikubenamespace:kube-systemuser:tls-bootstrap-token-username:tls-bootstrap-token-user@kubernetescurrent-context:tls-bootstrap-token-user@kuberneteskind:Configpreferences:{}users:- name:tls-bootstrap-token-useruser:token:ke3uxh.vhxb3ttj1nquno5tRunning Krustlet Once you have the bootstrap config in place, you can run Krustlet:\n$ KUBECONFIG=~/.krustlet/config/kubeconfig krustlet-wasi --port 3000 --bootstrap-file /path/to/your/bootstrap.conf Krustlet will begin the bootstrapping process, and then await manual certificate approval (described below) before launching.\nA couple important notes here. KUBECONFIG should almost always be set, especially in developer/local machine situations. During the bootstrap process, Krustlet will generate a kubeconfig with the credentials it obtains during the bootstrapping process and write it out to the specified location in KUBECONFIG. If a kubeconfig already exists there, it will be loaded and skip the bootstrapping process. A similar process occurs during the bootstrapping of the serving certificates, they will be written out to the paths specified by --cert-file (default $KRUSTLET_DATA_DIR/config/krustlet.crt) and --private-key-file (default $KRUSTLET_DATA_DIR/config/krustlet.key). If they already exist, then they will be loaded and bootstrapping skipped.\nApproving the serving CSR Once you have started Krustlet, there is one more manual step (though this could be automated depending on your setup). The client certs Krustlet needs are generally approved automatically by the API. However, the serving certs require manual approval. To do this, you’ll need the hostname you specified for the --hostname flag or the output of hostname if you didn’t specify anything. Then run:\n$ kubectl certificate approve \u003chostname\u003e-tls Once you do this, Krustlet will automatically grab the new certs and start running.\n","excerpt":"Bootstrapping Krustlet As of version 0.3.0, Krustlet supports …","ref":"/howto/bootstrapping/","title":""},{"body":"Registering a CSI Driver For more information on what is the Container Storage Interface and how it relates to a CSI driver, see the topic guide for more information.\nA Krustlet Provider with CSI support will check for new drivers registered to the plugins/ directory (by default, this is $HOME/.krustlet/plugins). You will need to inform your CSI driver to bind its socket at that location in order for a Provider to recognize and register the driver.\nYou will also need to install and run the following projects so that the PersistentVolumeClaim’s volume will be provisioned and readily available for use:\n node-driver-registrar external-provisioner  Do keep in mind that some CSI drivers rely on linux-specific command line tooling like mount, so these tools may only work on Linux. Cross-platform support is not guaranteed. Refer to the driver’s documentation for more information.\nHow do I use a CSI Volume? Assuming a CSI storage plugin is already deployed on your cluster, you can use it through familiar Kubernetes storage primitives: PersistentVolumeClaims, PersistentVolumes, and StorageClasses.\nA basic example to start with would be the host-path CSI driver. This project is not recommended for use in production, but will work as an example.\nTo start, we’ll need to create a StorageClass. A StorageClass provides a way for administrators to describe the “classes” of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is unopinionated about what classes represent. This concept is sometimes called “profiles” in other storage systems.\nThe following StorageClass enables dynamic creation of “csi-hostpath-sc” volumes by a CSI volume plugin called “hostpath.csi.k8s.io”. This storage class will also allow for volume expansion.\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:csi-hostpath-scprovisioner:hostpath.csi.k8s.ioreclaimPolicy:DeletevolumeBindingMode:ImmediateallowVolumeExpansion:trueAfter installing a StorageClass, we can start creating PersistentVolumeClaims to provision and prepare volumes. These will be mounted to Kubernetes Pods that request it as a volume. Note how the PVC requests the same StorageClass we created earlier.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:csi-pvcspec:accessModes:- ReadWriteOnceresources:requests:storage:1GistorageClassName:csi-hostpath-scA Pod can then request that volume to be mounted by using the volumes API.\nkind:PodapiVersion:v1metadata:name:my-frontendspec:containers:- name:my-frontendimage:example.com/my-frontend:v1.0.0volumeMounts:- mountPath:\"/data\"name:my-csi-volumevolumes:- name:my-csi-volumepersistentVolumeClaim:claimName:csi-pvcWhen the pod referencing a CSI volume is scheduled, Krustlet will trigger the appropriate operations against the external CSI plugin (ControllerPublishVolume, NodePublishVolume, etc.) to ensure the specified volume is attached, mounted, and ready to use by the containers in the pod.\nAddendum: Role-based Access Control In the event that the Pod is erroring saying that the kubelet doesn’t have the correct admission controls to access storage classes, you’ll have to create the following cluster role and role binding to allow the system:nodes group to access them:\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:storageclass-readerrules:- apiGroups:[\"storage.k8s.io\"]resources:[\"storageclasses\"]verbs:[\"get\",\"watch\",\"list\"]---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:node-storageclass-readersubjects:- kind:Groupname:system:nodesapiGroup:rbac.authorization.k8s.ioroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:storageclass-reader","excerpt":"Registering a CSI Driver For more information on what is the Container …","ref":"/howto/csi/","title":""},{"body":"Running Krustlet on Azure This guide demonstrates how to run Krustlet on Azure.\nPrerequisites This guide will require both the Azure CLI as well as kubectl to connect to the cluster after it has been provisioned.\nThis specific tutorial will run Krustlet on another Azure Virtual Machine within the same Virtual Network as the Kubernetes cluster.\nStep 1: Creating a Service Principal Prior to deploying Krustlet, a Service Principal needs to exist.\nThe following Azure CLI command can be used to create a Service Principal. Make sure to change ServicePrincipalName with your own unique name for a Service Principal.\n$ az ad sp create-for-rbac --name ServicePrincipalName --skip-assignment The output for a service principal with password authentication includes the password key. Make sure you copy this value - it can’t be retrieved. If you forget the password, reset the service principal credentials.\nThe ID and password appears in the output of az ad sp create-for-rbac and are used in the ARM template’s parameters. Make sure to record their values for later use.\nStep 2: Generate an SSH Key You will also need to generate an SSH key. This will be used to SSH into the machines for debugging purposes… Or if you’re just curious and want to see how the sausage is made.\nFollow the guide on Github to generate a new SSH key.\nStep 3: Click the Button Go ahead. Click the button.\n\nCopy the content of your public key and paste it into the “SSH Public Key” parameter. Also make sure to copy and paste the content of your Service Principal’s client ID (appID) and password into the parameters.\nStep 3(b): Troubleshooting In case the deployment fails, inspect the raw error message Azure reports from the failed deployment. The deployment could fail due to a name collision, and the raw error logs could help determine what went wrong.\nStep 4: Test that things work Once the cluster has been deployed, now you can see things work! Connect to the cluster and feel free to give any of the demos a try like so:\n$ az aks get-credentials --name krustlet --resource-group my-resource-group $ kubectl apply -f demos/wasi/hello-world-rust/k8s.yaml # wait a few seconds for the pod to run $ kubectl logs hello-world-wasi-rust hello from stdout! hello from stderr! CONFIG_MAP_VAL=cool stuff FOO=bar POD_NAME=hello-world-wasi-rust Args are: [] Bacon ipsum dolor amet chuck turducken porchetta, tri-tip spare ribs t-bone ham hock. Meatloaf pork belly leberkas, ham beef pig corned beef boudin ground round meatball alcatra jerky. Pancetta brisket pastrami, flank pork chop ball tip short loin burgdoggen. Tri-tip kevin shoulder cow andouille. Prosciutto chislic cupim, short ribs venison jerky beef ribs ham hock short loin fatback. Bresaola meatloaf capicola pancetta, prosciutto chicken landjaeger andouille swine kielbasa drumstick cupim tenderloin chuck shank. Flank jowl leberkas turducken ham tongue beef ribs shankle meatloaf drumstick pork t-bone frankfurter tri-tip. Step 5: Tear it down After you’re done testing, delete the cluster with\n$ az group delete -n my-resource-group ","excerpt":"Running Krustlet on Azure This guide demonstrates how to run Krustlet …","ref":"/howto/krustlet-on-azure/","title":""},{"body":"Running Krustlet on Managed Kubernetes on DigitalOcean These steps are for running a Krustlet node on a DigitalOcean Droplet in a Managed Kubernetes DigitalOcean cluster.\nPrerequisites You will require a Managed Kubernetes on DigitalOcean cluster. See the how-to guide for running Managed Kubernetes on DigitalOcean for more information.\nThis tutorial runs Krustlet on a DigitalOcean Droplet (VM); however you may follow these steps from any device that can start a web server on an IP accessible from the Kubernetes control plane.\nIn the how-to guide for running Managed Kubernetes on DigitalOcean, several environment variables were used to define the cluster. Let’s reuse those values:\n$ CLUSTER=[[YOUR-CLUSTER-NAME]] $ VERSION=\"1.19.3-do.3\" $ SIZE=\"s-1vcpu-2gb\" $ REGION=\"sfo3\" Let’s also confirm that the cluster exists:\n$ doctl kubernetes cluster list Step 1: Create DigitalOcean Droplet (VM) As with the cluster, there are several values (size, region) that you will need to determine before you create the Droplet. doctl compute includes commands to help you determine slugs for these values:\n$ doctl compute size list $ doctl compute region list $ doctl compute image list --public If you’d prefer, you may use the values below. However, it is strongly recommended that you use SSH keys to authenticate with Droplets. DigitalOcean provides instructions.\nYou may then list your SSH keys:\n$ doctl compute ssh-key list  NOTE In this case, you reference the key using an ID value\n We can create a new DigitalOcean Droplet using the following command:\n$ INSTANCE=[[YOUR-INSTANCE-NAME]] $ SIZE=\"s-1vcpu-2gb\" # Need not be the same size as the cluster node(s) $ REGION=\"sfo3\" # Need not be the same region as the cluster IMAGE=\"debian-10-x64\" SSH_KEY=[[YOUR-SSH-KEY]] doctl compute droplet create ${INSTANCE} \\ --region ${REGION} \\ --size ${SIZE} \\ --ssh-keys ${SSH_KEY} \\ --tag-names krustlet,wasm \\ --image ${IMAGE}  NOTE The service will response with an ID value for the Droplet. As long as the Droplet name (INSTANCE) is unique, you may refer to the Droplet by its name value too.\n You will need the Droplet’s IPv4 public address so make a note of it (IP):\n$ doctl compute droplet get ${INSTANCE} \\ --format PublicIPv4 \\ --no-header Step 2: Get a bootstrap config for your Kustlet node Krustlet requires a bootstrap token and config the first time it runs. Follow the guide here, setting the CONFIG_DIR variable to ./, to generate a bootstrap config and then return to this document. If you already have a kubeconfig available that you generated through another process, you can proceed to the next step. However, the credentials Krustlet uses must be part of the system:nodes group in order for things to function properly.\nNOTE: You may be wondering why you can’t run this on the VM you just provisioned. We need access to the Kubernetes API in order to create the bootstrap token, so the script used to generate the bootstrap config needs to be run on a machine with the proper Kubernetes credentials.\nStep 3: Copy bootstrap config to Droplet The first thing we’ll need to do is copy up the assets we generated in steps 1 and 2. Copy them to the VM by typing:\nscp -i ${PRIVATE_KEY} \\ ${HOME}/.krustlet/config/bootstrap.conf \\ root@${IP}:.  NOTE IP is the Droplet’s IPv4 address from step #1 and PRIVATE_KEY is the location of the file containing the private (!) key that corresponds to the public key that you used when you created the Droplet.\n We can then SSH into the Droplet by typing:\n$ doctl compute ssh ${INSTANCE} \\ --ssh-key-path ${PRIVATE_KEY} If you’d prefer, you may use ssh directly:\n$ ssh -i ${PRIVATE_KEY} root@${IP} Step 4: Install and configure Kruslet Install the latest release of krustlet following the install guide.\nLet’s use the built-in krustlet-wasi provider:\n$ KUBECONFIG=${PWD}/kubeconfig ${PWD}/krustlet-wasi \\ --node-ip=${IP} \\ --node-name=\"krustlet\" \\ --bootstrap-file=${PWD}/bootstrap.conf \\ --cert-file=${PWD}/krustlet.crt \\ --private-key-file=${PWD}/krustlet.key  NOTE You’ll need the IP of the Droplet from step 1. NOTE To increase the level of debugging, you may prefix the command with RUST_LOG=info or RUST_LOG=debug.\n If you restart the Krustlet after successfully (!) bootstrapping, you may run:\n$ KUBECONFIG=${PWD}/kubeconfig ${PWD}/krustlet-wasi \\ --node-ip=${IP} \\ --node-name=\"krustlet\" \\ --cert-file=${PWD}/krustlet.crt \\ --private-key-file=${PWD}/krustlet.key If bootstrapping fails, you should delete the CSR and try to bootstrap again:\n$ kubectl delete csr ${INSTANCE}-tls Step 4a: Approving the serving CSR Once you have started Krustlet, there is one more manual step (though this could be automated depending on your setup) to perform. The client certs Krustlet needs are generally approved automatically by the API. However, the serving certs require manual approval. To do this, you’ll need the hostname you specified for the --hostname flag or the output of hostname if you didn’t specify anything. From another terminal that’s configured to access the cluster, run:\n$ kubectl certificate approve ${INSTANCE}-tls  NOTE You will only need to do this approval step the first time Krustlet starts. It will generate and save all of the needed credentials to your machine.\n You should be able to enumerate the cluster’s nodes including the Krustlet by typing:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION krustlet Ready \u003cnone\u003e 60s 0.5.0 ${CLUSTER}-default-pool-39yh5 Ready \u003cnone\u003e 10m v1.19.3 Step 5: Test that things work We may test that the Krustlet is working by running one of the demos:\n$ kubectl apply --filename=https://raw.githubusercontent.com/deislabs/krustlet/master/demos/wasi/hello-world-rust/k8s.yaml $ kubectl get pods NAME READY STATUS RESTARTS AGE hello-world-wasi-rust 0/1 ExitCode:0 0 12s $ kubectl logs pods/hello-world-wasi-rust hello from stdout! hello from stderr! FOO=bar CONFIG_MAP_VAL=cool stuff POD_NAME=hello-world-wasi-rust Args are: [] Bacon ipsum dolor amet chuck turducken porchetta, tri-tip spare ribs t-bone ham hock. Meatloaf pork belly leberkas, ham beef pig corned beef boudin ground round meatball alcatra jerky. Pancetta brisket pastrami, flank pork chop ball tip short loin burgdoggen. Tri-tip kevin shoulder cow andouille. Prosciutto chislic cupim, short ribs venison jerky beef ribs ham hock short loin fatback. Bresaola meatloaf capicola pancetta, prosciutto chicken landjaeger andouille swine kielbasa drumstick cupim tenderloin chuck shank. Flank jowl leberkas turducken ham tongue beef ribs shankle meatloaf drumstick pork t-bone frankfurter tri-tip. Step 6: Run Krustlet as a service Create krustlet.service in /etc/systemd/system/krustlet.service on the VM.\n[Unit] Description=Krustlet, a kubelet implementation for running WASM [Service] Restart=on-failure RestartSec=5s Environment=KUBECONFIG=/etc/krustlet/config/kubeconfig Environment=KRUSTLET_NODE_IP=[[REPLACE-WITH-IP]] Environment=KRUSTLET_NODE_NAME=krustlet Environment=KRUSTLET_CERT_FILE=/etc/krustlet/config/krustlet.crt Environment=KRUSTLET_PRIVATE_KEY_FILE=/etc/krustlet/config/krustlet.key Environment=KRUSTLET_DATA_DIR=/etc/krustlet Environment=RUST_LOG=wasi_provider=info,main=info ExecStart=/usr/local/bin/krustlet-wasi User=root Group=root [Install] WantedBy=multi-user.target Ensure that the krustlet.service has the correct ownership and permissions with:\n$ sudo chown root:root /etc/systemd/system/krustlet.service $ sudo chmod 644 /etc/systemd/system/krustlet.service Then:\n$ sudo mkdir -p /etc/krustlet/config \u0026\u0026 sudo chown -R root:root /etc/krustlet $ sudo mv {krustlet.*,kubeconfig} /etc/krustlet/config \u0026\u0026 chmod 600 /etc/krustlet/* Once you have done that, run the following commands to make sure the unit is configured to start on boot:\n$ sudo systemctl enable krustlet \u0026\u0026 sudo systemctl start krustlet You may confirm the status of the service and review logs using:\n$ systemctl status krustlet.service $ journalctl --unit=krustlet.service --follow ## Delete the VM When you are finished with the VM, you can delete it by typing: ```console $ doctl compute droplet delete ${INSTANCE} When you are finished with the cluster, you can delete it by typing:\n$ doctl kubernetes cluster delete ${CLUSTER} doctl kubernetes cluster delete will also attempt to delete the cluster’s configuration (cluster, context, user) from the default Kubernetes config file (Linux: ${HOME}/.kube/config). You will neeed to set a new default context.\n","excerpt":"Running Krustlet on Managed Kubernetes on DigitalOcean These steps are …","ref":"/howto/krustlet-on-do/","title":""},{"body":"Running Krustlet on Amazon Elastic Kubernetes Service (EKS) Currently, EKS does not support running managed node groups with custom Amazon Machine Images (AMI).\nHowever, it does appear the feature might be coming soon.\nUntil that time, we can use eksctl to create and manage a node group with a custom Krustlet-based AMI.\nPrerequisites The following tools are needed to complete this walkthrough:\n Amazon CLI - use aws configure to set your access keys and default region Packer eksctl  Building the Krustlet-based AMI We will be using Packer to spin up an EC2 instance to build the AMI.\nThere is a Makefile in docs/howto/assets/eks that will run packer for you. It will use a c5.2xlarge EC2 instance to build the AMI with. Use the instance_type variable to make to change the type of the EC2 instance used.\nRun make to build the AMI:\n$ cd docs/howto/assets/eks $ make You can also build the AMI with a different version of Krustlet from a forked repo. For example:\n$ cd docs/howto/assets/eks $ KRUSTLET_VERSION=$(git rev-parse --short HEAD) KRUSTLET_SRC=https://github.com/jingweno/krustlet/archive/$(git rev-parse --short HEAD).tar.gz make krustlet This command will take a while to build Krustlet from source on the EC2 instance. In the future, a prebuilt binary for Amazon Linux 2 might be available that would speed up the AMI creation process.\nIf everything works correctly, you should see the command complete with output similar to:\n... ==\u003e Builds finished. The artifacts of successful builds are: --\u003e amazon-ebs: AMIs were created: us-west-2: ami-07adf9ce893885a3d --\u003e amazon-ebs: Make note of the AMI identifier (in the example output above it would be ami-07adf9ce893885a3d) as it will be used to create the EKS cluster.\nCreating the EKS cluster We will be using eksctl to deploy the EKS cluster.\nCreate a file named cluster.yaml with the following contents, replacing the region and ami fields with your values:\napiVersion:eksctl.io/v1alpha5kind:ClusterConfigmetadata:name:krustlet-demoregion:\u003cYOUR_AWS_REGION_HERE\u003eversion:\"1.15\"nodeGroups:- name:krustletami:\u003cYOUR_AMI_HERE\u003einstanceType:t3.smallminSize:1maxSize:3desiredCapacity:2ssh:allow:trueoverrideBootstrapCommand:/etc/eks/bootstrap.sh --krustlet-node-labels \"alpha.eksctl.io/cluster-name=krustlet-demo,alpha.eksctl.io/nodegroup-name=krustlet\"This will create a EKS cluster named krustlet-demo with a single unmanaged node group named krustlet with two t3.small nodes.\nBe aware that the overrideBootstrapCommand setting is required to properly boot the nodes. Without it, the Krustlet service will not be started and the nodes will not automatically join the cluster.\nUse eksctl to create the cluster:\n$ eksctl create cluster -f cluster.yaml This command will take a long time to run as it provisions the EKS cluster and nodes.\nEventually, the command will be stuck on the following output:\n... [ℹ] waiting for at least 1 node(s) to become ready in \"krustlet\" With another shell, ensure the nodes have joined the cluster:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-24-34.us-west-2.compute.internal Ready agent 23s v1.17.0 ip-192-168-44-27.us-west-2.compute.internal Ready agent 17s v1.17.0 You should see two nodes with different names in the output.\nRunning a WebAssembly application Let’s deploy a demo WebAssembly application to the cluster:\n$ kubectl apply -f demos/wasi/hello-world-rust/k8s.yaml Check that the pod ran to completion:\n$ kubectl get pod hello-world-wasi-rust NAME READY STATUS RESTARTS AGE hello-world-wasi-rust 0/1 ExitCode:0 0 7s This output shows the pod completed with an exit code of 0.\nTake a look at the log to see the output of the application:\n$ kubectl logs hello-world-wasi-rust hello from stdout! hello from stderr! POD_NAME=hello-world-wasi-rust FOO=bar CONFIG_MAP_VAL=cool stuff Args are: [] Congratulations! You’ve run a WebAssembly program on an EKS cluster!\nDeleting the cluster Use eksctl to delete the cluster and the nodes:\n$ eksctl delete cluster --name krustlet-demo Deleting the Krustlet AMI Determine the snapshot identifier of the AMI, where $AMI_ID is the identifier of your Krustlet AMI:\n$ aws ec2 describe-images --image-ids $AMI_ID | grep SnapshotId Use aws to deregister the AMI, where $AMI_ID is the identifier of your Krustlet AMI:\n$ aws ec2 deregister-image --image-id $AMI_ID Next, delete the snapshot, where $SNAPSHOT_ID is the previously determined snapshot identifier:\n$ aws ec2 delete-snapshot --snapshot-id $SNAPSHOT_ID ","excerpt":"Running Krustlet on Amazon Elastic Kubernetes Service (EKS) Currently, …","ref":"/howto/krustlet-on-eks/","title":""},{"body":"Running Krustlet on Google Kubernetes Engine (GKE) These steps are for running a Krustlet node in a GKE cluster.\nPrerequisites You will require a GKE cluster. See the how-to guide for running Kubernetes on GKE for more information.\nThis specific tutorial will be running Krustlet on a Compute Engine VM; however you may follow these steps from any device that can start a web server on an IP accessible from the Kubernetes control plane.\nIn the how-to guide for running Kubernetes on GKE, several environment variables were used to define a Google Cloud Platform project, region and Kubernetes Engine cluster. Let’s reuse those values:\n$ PROJECT=[YOUR-PROJECT] # Perhaps $(whoami)-$(date +%y%m%d)-krustlet $ REGION=\"us-west1\" # Use a region close to you `gcloud compute regions list --project=${PROJECT}` $ CLUSTER=\"cluster\" Let’s confirm that the cluster exists. We can do this using either gcloud or kubectl:\n$ gcloud container clusters describe ${CLUSTER} --project=${PROJECT} --region=${REGION} $ gcloud container clusters describe ${CLUSTER} --project=${PROJECT} --region=${REGION} --format=\"value(status)\" RUNNING Or:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION gke-cluster-default-pool-1a3a5b85-scds Ready \u003cnone\u003e 1m v1.17.4-gke.10 gke-cluster-default-pool-3885c0e3-6zw2 Ready \u003cnone\u003e 1m v1.17.4-gke.10 gke-cluster-default-pool-6d70a85d-19r8 Ready \u003cnone\u003e 1m v1.17.4-gke.10  NOTE If you chose to create a single-zone cluster, replace --region=${REGION} with --zone=${ZONE} in the above gcloud commands.\n Step 1: Create Compute Engine VM We can create a new VM with the following command:\n$ INSTANCE=\"krustlet\" # Name of this VM must matches the certificate's CN $ # The cluster is distributed across the zones in the region $ # For the VM, we'll pick one of the zones $ ZONE=\"${REGION}-a\" # Pick one of the zones in this region $ gcloud beta compute instances create ${INSTANCE} \\ --project=${PROJECT} \\ --zone=${ZONE} \\ --machine-type \"n1-standard-1\" \\ --image-family=\"debian-10\" \\ --image-project=\"debian-cloud\" NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS krustlet us-west1-a n1-standard-1 xx.xx.xx.xx yy.yy.yy.yy RUNNING It should take less than 30-seconds to provision the VM.\nLet’s determine the instance’s internal (!) IP to use when creating the Kubernete certificate and subsequently running Krustlet. In step #4, you’ll need to copy this value into the command that is used to run Krustlet on the VM:\n$ IP=$(gcloud compute instances describe ${INSTANCE} \\ --project=${PROJECT} \\ --zone=${ZONE} \\ --format=\"value(networkInterfaces[0].networkIP)\") \u0026\u0026 echo ${IP} Step 2: Get a bootstrap config for your Krustlet node Krustlet requires a bootstrap token and config the first time it runs. Follow the guide here, setting the CONFIG_DIR variable to ./, to generate a bootstrap config and then return to this document. If you already have a kubeconfig available that you generated through another process, you can proceed to the next step. However, the credentials Krustlet uses must be part of the system:nodes group in order for things to function properly.\nNOTE: You may be wondering why you can’t run this on the VM you just provisioned. We need access to the Kubernetes API in order to create the bootstrap token, so the script used to generate the bootstrap config needs to be run on a machine with the proper Kubernetes credentials\nStep 3: Copy bootstrap config to VM The first thing we’ll need to do is copy up the assets we generated in steps 1 and 2. Copy them to the VM by typing:\n$ gcloud compute scp bootstrap.conf ${INSTANCE}: --project=${PROJECT} --zone=${ZONE} We can then SSH into the instance by typing:\n$ gcloud compute ssh ${INSTANCE} --project=${PROJECT} --zone=${ZONE} Step 4: Install and configure Krustlet Install the latest release of krustlet following the install guide.\nLet’s use the built-in krustlet-wasi provider:\n$ KUBECONFIG=${PWD}/kubeconfig krustlet-wasi \\ --hostname=\"krustlet\" \\ --node-ip=${IP} \\ --node-name=\"krustlet\" \\ --bootstrap-file=./bootstrap.conf NOTE To increase the level of debugging, you may prefix the command with RUST_LOG=info or RUST_LOG=debug.\nNOTE The value of ${IP} was determined in step #1.\nStep 4a: Approving the serving CSR Once you have started Krustlet, there is one more manual step (though this could be automated depending on your setup) to perform. The client certs Krustlet needs are generally approved automatically by the API. However, the serving certs require manual approval. To do this, you’ll need the hostname you specified for the --hostname flag or the output of hostname if you didn’t specify anything. From another terminal that’s configured to access the cluster, run:\n$ kubectl certificate approve \u003chostname\u003e-tls NOTE: You will only need to do this approval step the first time Krustlet starts. It will generate and save all of the needed credentials to your machine\nYou should be able to enumerate the cluster’s nodes including the Krustlet by typing:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION gke-cluster-default-pool-1a3a5b85-scds Ready \u003cnone\u003e 59m v1.17.4-gke.10 gke-cluster-default-pool-3885c0e3-6zw2 Ready \u003cnone\u003e 36m v1.17.4-gke.10 gke-cluster-default-pool-6d70a85d-19r8 Ready \u003cnone\u003e 59m v1.17.4-gke.10 krustlet Ready agent 8s v1.17.0 Step 5: Test that things work We may test that the Krustlet is working by running one of the demos:\n$ kubectl apply --filename=https://raw.githubusercontent.com/deislabs/krustlet/master/demos/wasi/hello-world-rust/k8s.yaml $ # wait a few seconds for the pod to run $ kubectl logs pods/hello-world-wasi-rust hello from stdout! hello from stderr! CONFIG_MAP_VAL=cool stuff FOO=bar POD_NAME=hello-world-wasi-rust Args are: []  NOTE you may receive an ErrImagePull and Failed to pull image and failed to generate container. This results if the taints do not apply correctly. You should be able to resolve this issue, using the following YAML.\n apiVersion:v1kind:ConfigMapmetadata:name:hello-world-wasi-rustdata:myval:\"cool stuff\"---apiVersion:v1kind:Podmetadata:name:hello-world-wasi-rustspec:containers:- name:hello-world-wasi-rustimage:webassembly.azurecr.io/hello-world-wasi-rust:v0.1.0env:- name:FOOvalue:bar- name:POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.name- name:CONFIG_MAP_VALvalueFrom:configMapKeyRef:key:myvalname:hello-world-wasi-rustnodeSelector:kubernetes.io/arch:\"wasm32-wasi\"tolerations:- key:\"kubernetes.io/arch\"operator:\"Equal\"value:\"wasm32-wasi\"effect:\"NoExecute\"- key:\"kubernetes.io/arch\"operator:\"Equal\"value:\"wasm32-wasi\"effect:\"NoSchedule\"- key:\"node.kubernetes.io/network-unavailable\"operator:\"Exists\"effect:\"NoSchedule\"Step 6: Run Krustlet as a service Create krustlet.service in /etc/systemd/system/krustlet.service on the VM.\n[Unit] Description=Krustlet, a kubelet implementation for running WASM [Service] Restart=on-failure RestartSec=5s Environment=KUBECONFIG=/etc/krustlet/config/kubeconfig Environment=NODE_NAME=krustlet Environment=KRUSTLET_CERT_FILE=/etc/krustlet/config/krustlet.crt Environment=KRUSTLET_PRIVATE_KEY_FILE=/etc/krustlet/config/krustlet.key Environment=KRUSTLET_DATA_DIR=/etc/krustlet Environment=RUST_LOG=wasi_provider=info,main=info Environment=KRUSTLET_BOOTSTRAP_FILE=/etc/krustlet/config/bootstrap.conf ExecStart=/usr/local/bin/krustlet-wasi User=root Group=root [Install] WantedBy=multi-user.target Ensure that the krustlet.service has the correct ownership and permissions with:\n$ sudo chown root:root /etc/systemd/system/krustlet.service $ sudo chmod 644 /etc/systemd/system/krustlet.service Then:\n$ sudo mkdir -p /etc/krustlet/config \u0026\u0026 sudo chown -R root:root /etc/krustlet $ sudo mv {krustlet.*,kubeconfig} /etc/krustlet \u0026\u0026 chmod 600 /etc/krustlet/* Once you have done that, run the following commands to make sure the unit is configured to start on boot:\n$ sudo systemctl enable krustlet \u0026\u0026 sudo systemctl start krustlet Delete the VM When you are finished with the VM, you can delete it by typing:\n$ gcloud compute instances delete ${INSTANCE} --project=${PROJECT} --zone=${ZONE} --quiet When you are finished with the cluster, you can delete it by typing:\n$ # If you created a regional cluster $ gcoud container clusters delete ${CLUSTER} --project=${PROJECT} --region=${REGION} $ # If you created a zonal cluster $ gcoud container clusters delete ${CLUSTER} --project=${PROJECT} --zone=${ZONE} ","excerpt":"Running Krustlet on Google Kubernetes Engine (GKE) These steps are for …","ref":"/howto/krustlet-on-gke/","title":""},{"body":"Running Krustlet on Kubernetes in Docker (KinD) This how-to guide demonstrates how to boot a Krustlet node in a KinD cluster.\nPrerequisites You will require a running KinD cluster for this how-to. kubectl is also required. See the how-to guide for running Kubernetes on KinD for more information.\nThis specific tutorial will be running Krustlet on your host Operating System; however, you can follow these steps from any device that can start a web server on an IP accessible from the Kubernetes control plane, including KinD itself.\nStep 1: Get a bootstrap config Krustlet requires a bootstrap token and config the first time it runs. Follow the guide here to generate a bootstrap config and then return to this document. If you already have a kubeconfig available that you generated through another process, you can proceed to the next step. However, the credentials Krustlet uses must be part of the system:nodes group in order for things to function properly.\nStep 2: Determine the default gateway The default gateway for most Docker containers (including your KinD host) is generally 172.17.0.1. We can use this IP address from the guest Operating System (the KinD host) to connect to the host Operating System (where Krustlet is running). If this was changed, check ip addr show docker0 from the host OS to determine the default gateway.\nSpecial note: Docker Desktop for Mac For Docker Desktop for Mac users, the docker0 bridge network is unreachable from the host network (and vice versa). However, the en0 host network is accessible from within the container.\nBecause the en0 network is the default network, Krustlet will bind to this IP address automatically. You should not need to pass a --node-ip flag to Krustlet.\nIn the event this does not appear to be the case (for example, when the hostname cannot resolve to this address), check which IP address you have for the en0 network:\n$ ifconfig en0 en0: flags=8863\u003cUP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST\u003e mtu 1500 options=400\u003cCHANNEL_IO\u003e ether 78:4f:43:8d:4f:55 inet6 fe80::1c20:1e66:6322:6ae9%en0 prefixlen 64 secured scopeid 0x5 inet 192.168.1.167 netmask 0xffffff00 broadcast 192.168.1.255 nd6 options=201\u003cPERFORMNUD,DAD\u003e media: autoselect status: active In this example, I should use 192.168.1.167.\nSpecial note: Docker on Hyper-V Linux VMs For Docker running on a Linux VM on a Windows host under Hyper-V, the default gateway is usually 172.18.0.1.\nStep 3: Install and run Krustlet First, install the latest release of Krustlet following the install guide.\nOnce you have done that, run the following commands to run Krustlet’s WASI provider:\n# Since you are running locally, this step is important. Otherwise krustlet will pick up on your # local config and not be able to update the node status properly $ export KUBECONFIG=~/.krustlet/config/kubeconfig $ krustlet-wasi --node-ip 172.17.0.1 --bootstrap-file=~/.krustlet/config/bootstrap.conf Step 3a: Approving the serving CSR Once you have started Krustlet, there is one more manual step (though this could be automated depending on your setup) to perform. The client certs Krustlet needs are generally approved automatically by the API. However, the serving certs require manual approval. To do this, you’ll need the hostname you specified for the --hostname flag or the output of hostname if you didn’t specify anything. From another terminal that’s configured to access the cluster, run:\n$ kubectl certificate approve \u003chostname\u003e-tls NOTE: You will only need to do this approval step the first time Krustlet starts. It will generate and save all of the needed credentials to your machine\nThen, run kubectl get nodes -o wide and you should see output that looks similar to below:\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kind-control-plane Ready master 3m46s v1.17.0 172.17.0.2 \u003cnone\u003e Ubuntu 19.10 5.3.0-42-generic containerd://1.3.2 krustlet Ready agent 10s v1.17.0 172.17.0.1 \u003cnone\u003e \u003cunknown\u003e \u003cunknown\u003e mvp ","excerpt":"Running Krustlet on Kubernetes in Docker (KinD) This how-to guide …","ref":"/howto/krustlet-on-kind/","title":""},{"body":"Running Krustlet on MicroK8s These are steps for running Krustlet node(s) and MicroK8s on the same machine.\nPrerequisites You will require a running MicroK8s cluster for this guide. The steps below assume you will run MicroK8s and the Krustlet, on a single machine. kubectl is required but is installed with MicroK8s as microk8s.kubectl. The following instructions use microk8s.kubectl for simplicity. You may use a standlone kubectl if you prefer.\nIn order for the bootstrap authentication token to work, your kube-apiserver needs to have the --enable-bootstrap-token-auth feature flag enabled. See bootstrap-tokens for more information.\nTo verify you have the bootstrap authentication feature enabled, check the process args:\n$ ps -ef | grep kube-apiserver | grep \"enable-bootstrap-token-auth\" If it doesn’t show up and you installed using snap, you can find the startup args in /var/snap/microk8s/current/args/kube-apiserver and add the flag.\nNow you need to restart the kube-apiserver with the command:\n$ systemctl restart snap.microk8s.daemon-apiserver Step 1: Get a bootstrap config Krustlet requires a bootstrap token and config the first time it runs. Follow the guide here to generate a bootstrap config and then return to this document. This will If you already have a kubeconfig available that you generated through another process, you can proceed to the next step. However, the credentials Krustlet uses must be part of the system:nodes group in order for things to function properly.\n NOTE You should now have a file bootstrap.conf in ${HOME}/.krustlet/config\n Step 2: Install and configure Krustlet Install the latest release of Krustlet following the install guide.\nLet’s use the built in krustlet-wasi provider\n$ ./KUBECONFIG=${PWD}/krustlet-config \\ krustlet-wasi \\ --node-ip=127.0.0.1 \\ --node-name=krustlet \\ --bootstrap-file=${HOME}/.krustlet/config/bootstrap.conf  NOTE: To avoid the Krustlet using your default Kubernetes credentials (~/.kube/config), it is a good idea to override the default value here using KUBECONFIG. For bootstrapping, KUBECONFIG must point to a non-existent file (!). Bootstrapping will write a new configuration file to this location for you. NOTE: If you receive an error that the CSR already exists, you may safely delete the existing CSR (kubectl delete csr \u003chostname\u003e-tls) and try again.\n Step 2a: Approving the serving CSR Once you have started Krustlet, there is one more manual step (though this could be automated depending on your setup) to perform. The client certs Krustlet needs are generally approved automatically by the API. However, the serving certs require manual approval. To do this, you’ll need the hostname you specified for the --hostname flag or the output of hostname if you didn’t specify anything. From another terminal that’s configured to access the cluster, run:\n$ microk8s.kubectl certificate approve \u003chostname\u003e-tls  NOTE: You will only need to do this approval step the first time Krustlet starts. It will generate and save all of the needed credentials to your machine\n Step 3: Test that things work Now you can see things work! Feel free to give any of the demos a try in another terminal like so:\n$ microk8s.kubectl apply --filename=https://raw.githubusercontent.com/deislabs/krustlet/master/demos/wasi/hello-world-rust/k8s.yaml $ microk8s.kubectl logs pod/hello-world-wasi-rust hello from stdout! hello from stderr! POD_NAME=hello-world-wasi-rust FOO=bar CONFIG_MAP_VAL=cool stuff Args are: [] Bacon ipsum dolor amet chuck turducken porchetta, tri-tip spare ribs t-bone ham hock. Meatloaf pork belly leberkas, ham beef pig corned beef boudin ground round meatball alcatra jerky. Pancetta brisket pastrami, flank pork chop ball tip short loin burgdoggen. Tri-tip kevin shoulder cow andouille. Prosciutto chislic cupim, short ribs venison jerky beef ribs ham hock short loin fatback. Bresaola meatloaf capicola pancetta, prosciutto chicken landjaeger andouille swine kielbasa drumstick cupim tenderloin chuck shank. Flank jowl leberkas turducken ham tongue beef ribs shankle meatloaf drumstick pork t-bone frankfurter tri-tip. ","excerpt":"Running Krustlet on MicroK8s These are steps for running Krustlet …","ref":"/howto/krustlet-on-microk8s/","title":""},{"body":"Running Krustlet on Minikube This how-to guide demonstrates how to boot a Krustlet node in a Minikube cluster.\nPrerequisites You will require a running Minikube cluster for this how-to. The steps below assume that minikube was booted with the VirtualBox driver, though other drivers can be used with some changes. kubectl is also required.\nSee the how-to guide for running Kubernetes on Minikube for more information.\nThis specific tutorial will be running Krustlet on your host Operating System; however, you can follow these steps from any device that can start a web server on an IP accessible from the Kubernetes control plane, including Minikube itself.\nStep 1: Get a bootstrap config Krustlet requires a bootstrap token and config the first time it runs. Follow the guide here to generate a bootstrap config and then return to this document. This will If you already have a kubeconfig available that you generated through another process, you can proceed to the next step. However, the credentials Krustlet uses must be part of the system:nodes group in order for things to function properly.\nStep 2: Determine the default gateway The default gateway when you set up minikube with the VirtualBox driver is generally 10.0.2.2. We can use this IP address from the guest Operating System (the minikube host) to connect to the host Operating System (where Krustlet is running). If this was changed, use minikube ssh and ip addr show from the guest OS to determine the default gateway.\nStep 3: Install and run Krustlet First, install the latest release of Krustlet following the install guide.\nOnce you have done that, run the following commands to run Krustlet’s WASI provider:\n# Since you are running locally, this step is important. Otherwise krustlet will pick up on your # local config and not be able to update the node status properly $ export KUBECONFIG=~/.krustlet/config/kubeconfig $ krustlet-wasi --node-ip 10.0.2.2 --bootstrap-file=~/.krustlet/config/bootstrap.conf Step 3a: Approving the serving CSR Once you have started Krustlet, there is one more manual step (though this could be automated depending on your setup) to perform. The client certs Krustlet needs are generally approved automatically by the API. However, the serving certs require manual approval. To do this, you’ll need the hostname you specified for the --hostname flag or the output of hostname if you didn’t specify anything. From another terminal that’s configured to access the cluster, run:\n$ kubectl certificate approve \u003chostname\u003e-tls NOTE: You will only need to do this approval step the first time Krustlet starts. It will generate and save all of the needed credentials to your machine\nIn another terminal, run kubectl get nodes -o wide and you should see output that looks similar to below:\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready master 18m v1.18.0 192.168.99.165 \u003cnone\u003e Buildroot 2019.02.10 4.19.107 docker://19.3.8 krustlet Ready agent 9s v1.17.0 10.0.2.2 \u003cnone\u003e \u003cunknown\u003e \u003cunknown\u003e mvp ","excerpt":"Running Krustlet on Minikube This how-to guide demonstrates how to …","ref":"/howto/krustlet-on-minikube/","title":""},{"body":"Running Krustlet on WSL2 with Docker Desktop This how-to guide demonstrates how to boot a Krustlet node in Docker Desktop for Windows with WSL2 backend.\nInformation This tutorial will work on current Windows 10 Insider Slow ring and Docker Desktop for Windows stable release.\nThis tutorial will work on current Windows 10 Insider Slow ring and Docker Desktop for Windows stable release.\nConcerning Windows, this tutorial should work on the Production ring once it will be available.\nLast but not least, this will work on Windows 10 Home edition.\nPrerequisites You will require a WSL2 distro and Docker Desktop for Windows for this how-to. The WSL2 backend and Kubernetes features will need to be also enabled. See the Docker Desktop for Windows \u003e Getting started \u003e Kubernetes howto for more information.\nThis specific tutorial will be running Krustlet on your WSL2 distro and will explain how to access it from Windows.\nStep 1: Determine the default gateway The default gateway for most Docker containers is generally 172.17.0.1. This IP is only reachable, by default, from the WSL2 distro. However, the eth0 host network is accessible from Windows, so we can use this IP address to connect to the WSL2 distro (where Krustlet is running).\nIf this was changed, check ifconfig eth0 from the host OS to determine the default gateway:\n$ ifconfig eth0 eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 172.26.47.208 netmask 255.255.240.0 broadcast 172.26.47.255 inet6 fe80::215:5dff:fe98:ce48 prefixlen 64 scopeid 0x20\u003clink\u003e ether 00:15:5d:98:ce:48 txqueuelen 1000 (Ethernet) RX packets 16818 bytes 11576089 (11.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1093 bytes 115724 (113.0 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 In this example, I should use 172.26.47.208.\n TIP: get the IP from eth0\n $ export mainIP=$(ifconfig eth0 | grep \"inet \" | awk '{ print $2 }') The hostname being “applied” from Windows, the default hostname will not resolve to this address, therefore you need to pass the --node-ip and --node-name flag to Krustlet.\nStep 2: Get a bootstrap config Krustlet requires a bootstrap token and config the first time it runs. Follow the guide here to generate a bootstrap config and then return to this document. This will If you already have a kubeconfig available that you generated through another process, you can proceed to the next step. However, the credentials Krustlet uses must be part of the system:nodes group in order for things to function properly.\nStep 3: Install and run Krustlet First, install the latest release of Krustlet following the install guide.\nSecond, ensure the Kubernetes context is correctly set to docker-desktop:\n$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * docker-desktop docker-desktop docker-desktop # Optional if the context is not set correctly $ kubectl config set-context docker-desktop Context \"docker-desktop\" modified. Once you have done that, run the following commands to run Krustlet’s WASI provider:\n# Since you are running locally, this step is important. Otherwise krustlet will pick up on your # local config and not be able to update the node status properly $ export KUBECONFIG=~/.krustlet/config/kubeconfig $ krustlet-wasi --node-ip $mainIP --node-name krustlet --bootstrap-file=~/.krustlet/config/bootstrap.conf Step 3a: Approving the serving CSR Once you have started Krustlet, there is one more manual step (though this could be automated depending on your setup) to perform. The client certs Krustlet needs are generally approved automatically by the API. However, the serving certs require manual approval. To do this, you’ll need the hostname you specified for the --hostname flag or the output of hostname if you didn’t specify anything. From another terminal that’s configured to access the cluster, run:\n$ kubectl certificate approve \u003chostname\u003e-tls NOTE: You will only need to do this approval step the first time Krustlet starts. It will generate and save all of the needed credentials to your machine\nIn another terminal, run kubectl get nodes -o wide and you should see output that looks similar to below:\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME docker-desktop Ready master 3d23h v1.15.5 192.168.65.3 \u003cnone\u003e Docker Desktop 4.19.104-microsoft-standard docker://19.3.8 krustlet Ready agent 34s v1.17.0 172.26.47.208 \u003cnone\u003e \u003cunknown\u003e \u003cunknown\u003e mvp Optional: Delete the Krustlet node Once you will no more need the Krustlet node, you can remove it from your cluster with the following kubectl delete node command:\n$ kubectl delete node krustlet node \"krustlet\" deleted $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME docker-desktop Ready master 4d v1.15.5 192.168.65.3 \u003cnone\u003e Docker Desktop 4.19.104-microsoft-standard docker://19.3.8 Troubleshooting WASM workloads on Docker Desktop Docker Desktop’s Kubernetes always provides a schedulable node called docker-desktop. This node uses Docker to run containers. If you want to run WASM workloads on Krustlet, you must prevent these pods from being scheduled to the docker-desktop node. You can do this using a nodeSelector in pod specs. See Running WASM workloads for details.\n","excerpt":"Running Krustlet on WSL2 with Docker Desktop This how-to guide …","ref":"/howto/krustlet-on-wsl2/","title":""},{"body":"Running Krustlet on Kubernetes with inlets These are steps for running a krustlet node on your own computer. You can run any kind of Kubernetes cluster you like, whether that’s local on your computer or remote in a managed Kubernetes offering.\nThe instructions provided in this guide were contributed by Alex Ellis. For support with the instructions, see the #inlets channel of OpenFaaS Slack or raise a GitHub issue and tag @alexellis.\nPrerequisites There are a number of ways to give the Kubernetes API server access to the krustlet’s API. Various methods include using a VPN, a VM within the Kubernetes cluster’s private network, or a tunnel. Inlets is a popular open source service tunnel and proxy. It can be used to forward the port of the krustlet to the Kubernetes cluster so that the Kubernetes API server can access it as if it were deployed within the cluster directly.\nThe tunnel has two components: A client which runs on your local machine, and a server which is deployed as a Pod inside the Kubernetes cluster. The client connects to the server and provides a persistent link.\nDownload the latest release of the inlets binary from the project release page.\nMove the binary to /usr/local/bin, or place it somewhere on your $PATH.\nStep 1: Get a bootstrap config Krustlet requires a bootstrap token and config the first time it runs. Follow the guide here to generate a bootstrap config and then return to this document. This will If you already have a kubeconfig available that you generated through another process, you can proceed to the next step. However, the credentials Krustlet uses must be part of the system:nodes group in order for things to function properly.\nStep 2: Create the inlets service In order to start Krustlet with the correct node IP address, you’ll need to create the inlets service in Kubernetes like so:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: inlets labels: app: inlets spec: type: ClusterIP ports: - port: 8000 protocol: TCP targetPort: 8000 name: control - port: 3000 protocol: TCP targetPort: 3000 name: data selector: app: inlets EOF Once it has been created, run the following command to have the node IP available for next steps. This is a stable IP and won’t change.\nexport NODE_IP=$(kubectl get service inlets -o jsonpath=\"{.spec.clusterIP}\") Step 3: Run krustlet You’ll need the certificates generated from the bootstrap process for our next steps, so go ahead and start krustlet:\n# Since you are running locally, this step is important. Otherwise krustlet will pick up on your # local config and not be able to update the node status properly export KUBECONFIG=~/.krustlet/config/kubeconfig krustlet-wasi --node-ip $NODE_IP --bootstrap-file=~/.krustlet/config/bootstrap.conf Then open another terminal for the next steps.\nStep 4: Setup inlets server Create a Kubernetes secret for the inlets server:\nexport TOKEN=$(head -c 16 /dev/urandom |shasum|cut -d- -f1) echo $TOKEN \u003e token.txt kubectl create secret generic inlets-token --from-literal token=${TOKEN} Then, create a Kubernetes secret for krustlet’s TLS certificates. These will be used by the inlets server so that the kubelet can access the tunnel using the expected TLS certificates.\nkubectl create secret ghosttunnel-tls generic \\ --from-file tls.crt=~/.krustlet/config/krustlet.crt \\ --from-file tls.key=~/.krustlet/config/krustlet.key The inlets OSS version exposes services with HTTP within the cluster, so this example uses ghosttunnel as a tiny reverse proxy to mount the krustlet’s TLS certificates so that the kubelet gets a valid HTTPS response. The service created before will expose it to the cluster\n$ cat \u003c\u003cEOF | kubectl apply -f -apiVersion:apps/v1kind:Deploymentmetadata:name:inletsspec:replicas:1selector:matchLabels:app:inletstemplate:metadata:labels:app:inletsspec:volumes:- name:ghosttunnel-tls-volumesecret:secretName:ghosttunnel-tls- name:inlets-token-volumesecret:secretName:inlets-tokencontainers:- name:inletsimage:inlets/inlets:2.7.0imagePullPolicy:Alwayscommand:[\"inlets\"]args:- \"server\"- \"--token-from=/var/inlets/token\"- \"--control-port=8000\"- \"--port=3001\"volumeMounts:- name:inlets-token-volumemountPath:/var/inlets/- name:ghosttunnelimage:squareup/ghostunnel:v1.5.2imagePullPolicy:Alwaysargs:- \"server\"- \"--target=127.0.0.1:3001\"- \"--listen=0.0.0.0:3000\"- \"--cert=/etc/tls/tls.crt\"- \"--key=/etc/tls/tls.key\"- \"--disable-authentication\"volumeMounts:- name:ghosttunnel-tls-volumemountPath:/etc/tlsEOFStep 5: Run the inlets client Port-forward or expose the inlets server with:\nkubectl port-forward svc/inlets 8000:8000 \u0026 You can also expose inlets via Ingress using cert-manager to give its control-port a TLS certificate.\nRun the inlets client on your computer:\ninlets client \\ --upstream https://127.0.0.1:3000 \\ --remote ws://127.0.0.1:8000 --token $(token.txt) Step 6: Verify the node is available Show that the krustlet node has joined the cluster:\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME pool-3xbltttyc-3no2p Ready \u003cnone\u003e 153m v1.16.6 10.131.25.141 206.189.19.185 Debian GNU/Linux 9 (stretch) 4.19.0-0.bpo.6-amd64 docker://18.9.2 pool-3xbltttyc-3no2s Ready \u003cnone\u003e 153m v1.16.6 10.131.28.223 206.189.123.184 Debian GNU/Linux 9 (stretch) 4.19.0-0.bpo.6-amd64 docker://18.9.2 krustlet Ready agent 43m v1.17.0 10.245.157.226 \u003cnone\u003e \u003cunknown\u003e \u003cunknown\u003e mvp Appendix Remove the port-forward for inlets OSS We are using a port-forward to make it easier to use the tutorial. For permanent use, you will want to expose the inlets server and its control port directly. The OSS version can be configured with TLS, but this is not built-in.\nYou can set up an Ingress rule for the control-port of the inlets server (port 8000), and obtain a TLS certificate from LetsEncrypt.\nUse inlets PRO instead Inlets OSS is an L7 proxy that requires additional work to configure for krustlet. inlets PRO is a pure L4 TCP proxy with built-in TLS for the control-plane.\nWith inlets PRO you can expose the control port (8123) directly to the Internet as a NodePort, or LoadBalancer, or if you wish via an Ingress definition. The control port already has TLS configured, so won’t need additional link-layer encryption.\n","excerpt":"Running Krustlet on Kubernetes with inlets These are steps for running …","ref":"/howto/krustlet-with-inlets/","title":""},{"body":"Managed Kubernetes on DigitalOcean Managed Kubernetes on DigitalOcean is an inexpensive, professionally-managed Kubernetes service.\nYou’ll need a DigitalOcean account and will need to have provided a payment method. DigitalOcean offers a free trial.\nPrerequisites You may provision Kubernetes clusters and Droplets (DigitalOcean VMs) using the console but DigitalOcean’s CLI doctl is comprehensive and recommended. The instructions that follow assume you’ve installed doctl and authenticated to a DigitalOcean account.\n NOTE If you use the doctl Snap, consider connecting kubectl and ssh-keys to simplify the experience.\n Create Managed Kubernetes cluster doctl kubernetes includes commands for provisioning clusters. In order to create a cluster, you’ll need to provide a Kubernetes version, a node instance size, a DigitalOcean region and the number of nodes. Values for some of the values may be obtained using the following doctl kubernetes commands:\n$ doctl kubernetes options versions $ doctl kubernetes options regions $ doctl kubernetes options sizes  NOTE DigitalOcean uses unique identifiers called “slugs”. “slugs” are the identifiers used as values in many of doctl’s commands, e.g. 1.19.3-do.3 is the slug for Kubernetes version 1.19.3.\n If you’d prefer to use some reasonable default values, you may use the following command to create a cluster in DigitalOcean’s San Francisco region, using Kubernetes 1.19.3 with a single worker node (the master node is free). The worker node has 1 vCPU and 2GB RAM (currently $10/month).\n$ CLUSTER=[[YOUR-CLUSTER-NAME]] $ VERSION=\"1.19.3-do.3\" $ SIZE=\"s-1vcpu-2gb\" $ REGION=\"sfo3\" $ doctl kubernetes cluster create ${CLUSTER} \\ --auto-upgrade \\ --count 1 \\ --version ${VERSION} \\ --size ${SIZE} \\ --region ${REGION} doctl kubernetes cluster create should automatically update your default Kubernetes config (Linux: ${HOME}/.kube/config). doctl kubernetes cluster delete will remove this entry when it deletes the cluster. You should be able to:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION ${CLUSTER}-default-pool-39yh5 Ready \u003cnone\u003e 1m v1.19.3 Delete Managed Kubernetes cluster When you are finished with the cluster, you may delete it with:\n$ doctl kubernetes cluster delete ${CLUSTER}  NOTE This command should (!) delete the cluster’s entries (context, user) from the default Kubernetes config (Linux `{$HOME}/.kube/config) too.\n To confirm that the cluster has been deleted, if you try listing the clusters, the cluster you deleted should no longer be present:\n$ doctl kubernetes cluster list Or you may confirm that the Droplets have been deleted:\n$ doctl compute droplet list ","excerpt":"Managed Kubernetes on DigitalOcean Managed Kubernetes on DigitalOcean …","ref":"/howto/kubernetes-on-do/","title":""},{"body":"Running Kubernetes on Google Kubernetes Engine (GKE) Google Kubernetes Engine (GKE) is a secured and managed Kubernetes service.\nIf you haven’t used Google Cloud Platform, you’ll need a Google (e.g. Gmail) account. As a new customer, you may benefit from $300 free credit. Google Cloud Platform includes always free products. See Google Cloud Platform Free Tier\nPrerequisites You should be able to run Google Cloud SDK command-line tool gcloud. This is used to provision resources in Google Cloud Platform including Kubernetes clusters.\nEither install Google Cloud SDK or open a Cloud Shell.\nGoogle Cloud SDK is available for Linux, Windows and Mac OS. The instructions that follow document using the command-line on Linux. There may be subtle changes for Windows and Mac OS.\nGoogle Cloud Platform provides a browser-based Console. This is generally functionally equivalent to the command-line tool. The instructions that follow document using the command-line tool but you may perform these steps using the Console too.\nYou will also need Kubernetes command-line tool kubectl. kubectl is used by all Kubernetes distributions. So, if you’ve created Kubernetes clusters locally or on other cloud platforms, you may already have this tool installed. See Install and Set Up kubectl for instructions.\nConfigure Google Cloud CLI After installing Google Cloud SDK, you will need to initialize the tool. This also authenticates your account using a Google identity (e.g. Gmail). Do this by typing gcloud init. If for any reason, you have already run gcloud init, you may reauthenticate using gcloud auth login or check authentication with gcloud auth list.\nCreate GKE cluster Google Cloud Platform resources are aggregated by projects. Projects are assigned to Billing Accounts. GKE uses Compute Engine VMs as nodes and Compute Engine VMs require that assign a Billing Account to our project so that we may pay for the VMs.\n$ PROJECT=[YOUR-PROJECT] # Perhaps $(whoami)-$(date +%y%m%d)-krustlet $ BILLING=[YOUR-BILLING] # You may list these using `gcloud beta billing accounts list` $ # Create Project and assing Billing Account $ gcloud projects create ${PROJECT} $ gcloud alpha billing projects link ${PROJECT} --billing-account=${BILLING} $ # Enable Kubernetes Engine \u0026 Compute Engine $ gcloud services enable container.googleapis.com --project=${PROJECT} $ gcloud services enable compute.googleapis.com --project=${PROJECT} $ REGION=\"us-west1\" # Use a region close to you `gcloud compute regions list --project=${PROJECT}` $ CLUSTER=\"cluster\" $ # Create GKE cluster with 3 nodes (one per zone in the region) $ gcloud beta container clusters create ${CLUSTER} \\ --project=${PROJECT} \\ --region=${REGION} \\ --no-enable-basic-auth \\ --release-channel \"rapid\" \\ --machine-type \"n1-standard-1\" \\ --image-type \"COS_CONTAINERD\" \\ --preemptible \\ --num-nodes=\"1\"  NOTE This creates a cluster with nodes distributed across multiple zones in a region. This increases the cluster’s availability. If you’d prefer a less available (and cheaper) single zone cluster, you may use the following commands instead:\n $ ZONE=\"${REGION}-a\" # Or \"-b\" or \"-c\" $ gcloud beta container clusters create ${CLUSTER} \\ --project=${PROJECT} \\ --zone=${ZONE} \\ --no-enable-basic-auth \\ --release-channel \"rapid\" \\ --machine-type \"n1-standard-1\" \\ --image-type \"COS_CONTAINERD\" \\ --preemptible \\ --num-nodes=\"1\" After a minute, you should see the cluster created:\nNAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS cluster us-west1 1.17.4-gke.10 xx.xx.xx.xx n1-standard-1 1.17.4-gke.10 3 RUNNING  NOTE You may also use Cloud Console to interact with the cluster: https://console.cloud.google.com/ NOTE gcloud clusters create also configures kubectl to be able to access the cluster.\n You may confirm access to the cluster by typing:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION gke-cluster-default-pool-1a3a5b85-scds Ready \u003cnone\u003e 10m v1.17.4-gke.10 gke-cluster-default-pool-3885c0e3-6zw2 Ready \u003cnone\u003e 10m v1.17.4-gke.10 gke-cluster-default-pool-6d70a85d-19r8 Ready \u003cnone\u003e 10m v1.17.4-gke.10 You may confirm the Kubernetes configuration either by:\n$ more ${HOME}/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1C... server: https://xx.xx.xx.xx name: gke_${PROJECT}_${REGION}_${CLUSTER} contexts: - context: cluster: gke_${PROJECT}_${REGION}_${CLUSTER} user: gke_${PROJECT}_${REGION}_${CLUSTER} name: gke_${PROJECT}_${REGION}_${CLUSTER} current-context: gke_${PROJECT}_${REGION}_${CLUSTER} kind: Config preferences: {} users: - name: gke_${PROJECT}_${REGION}_${CLUSTER} user: auth-provider: config: cmd-args: config config-helper --format=json cmd-path: /snap/google-cloud-sdk/130/bin/gcloud expiry-key: '{.credential.token_expiry}' token-key: '{.credential.access_token}' name: gcp Or:\n$ kubectl config current-context gke_${PROJECT}_${REGION}_${CLUSTER} $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO * gke_${PROJECT}_${REGION}_${CLUSTER} gke_${PROJECT}_${REGION}_${CLUSTER} gke_${PROJECT}_${REGION}_${CLUSTER} Delete the Cluster When you are finished with the cluster, you may delete it with:\n$ gcloud beta container clusters delete ${CLUSTER} --project=${PROJECT} --region=${REGION} --quiet If you wish to delete everything in the project, you may delete hte project (including all its resources) with:\n$ gcloud projects delete ${PROJECT} --quiet  NOTE Both commands are irrevocable.\n ","excerpt":"Running Kubernetes on Google Kubernetes Engine (GKE) Google Kubernetes …","ref":"/howto/kubernetes-on-gke/","title":""},{"body":"Running Kubernetes on Kubernetes in Docker (KinD) This tutorial will focus on using a tool called kind, also known as “Kubernetes IN Docker”.\nIf you haven’t installed them already, go ahead and install Docker, install kind, and install kubectl.\nYou’ll need kubectl to interact with the cluster once it’s created.\nCreate a cluster Once Docker, kind, and kubectl are installed, create a cluster with kind:\n$ kind create cluster This will create a cluster with a single node - perfect for local development.\nYou should see output similar to the following:\nCreating cluster \"kind\" ... ✓ Ensuring node image (kindest/node:v1.17.0) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! 👋 Now we can interact with our cluster! Try that out now:\n$ kubectl cluster-info Kubernetes master is running at https://127.0.0.1:32768 KubeDNS is running at https://127.0.0.1:32768/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use ‘kubectl cluster-info dump’.\n","excerpt":"Running Kubernetes on Kubernetes in Docker (KinD) This tutorial will …","ref":"/howto/kubernetes-on-kind/","title":""},{"body":"Running Kubernetes on Minikube This tutorial will focus on using a tool called minikube.\nIf you haven’t installed them already, go ahead and install VirtualBox 5.2 or higher, install minikube, and install kubectl.\nYou’ll need kubectl to interact with the cluster once it’s created.\nCheck virtualization support To use VM drivers, verify that your system has virtualization support enabled:\n$ egrep -q 'vmx|svm' /proc/cpuinfo \u0026\u0026 echo yes || echo no If the above command outputs “no”:\n If you are running within a VM, your hypervisor does not allow nested virtualization. You will need to use the None (bare-metal) driver If you are running on a physical machine, ensure that your BIOS has hardware virtualization enabled  Create a cluster Once VirtualBox, minikube, and kubectl are installed, create a cluster with minikube:\n$ minikube start --driver=virtualbox This will create a cluster with a single node - perfect for local development.\nYou should see output similar to the following:\n😄 minikube v1.9.0 on Ubuntu 18.04 ✨ Using the virtualbox driver based on user configuration 💿 Downloading VM boot image ... 💾 Downloading Kubernetes v1.18.0 preload ... 🔥 Creating virtualbox VM (CPUs=2, Memory=6000MB, Disk=20000MB) ... 🐳 Preparing Kubernetes v1.18.0 on Docker 19.03.8 ... 🌟 Enabling addons: default-storageclass, storage-provisioner 🏄 Done! kubectl is now configured to use \"minikube\" Now we can interact with our cluster! Try that out now:\n$ kubectl cluster-info Kubernetes master is running at https://192.168.99.164:8443 KubeDNS is running at https://192.168.99.164:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use ‘kubectl cluster-info dump’.\n","excerpt":"Running Kubernetes on Minikube This tutorial will focus on using a …","ref":"/howto/kubernetes-on-minikube/","title":""},{"body":"Running Web Assembly (WASM) workloads in Kubernetes The Krustlet repository contains the built-in krustlet-wasi provider for running WASM workloads in Kubernetes. There is also a krustlet-wasmcloud provider available that also runs WASM workloads. These kubelets run workloads implemented as Web Assembly (WASM) modules rather than as OCI containers. Each running instance appears to Kubernetes as a node; Kubernetes schedules work to the instance as to any other node.\nRunning WASM modules on the right kubelet WASM modules are not interchangeable with OCI containers: krustlet-wasi and krustlet-wasmcloud can’t run OCI containers, and normal OCI nodes can’t run WASM modules. In order to run your WASM workloads on the right nodes, you should use the Kubernetes tolerations system; in some cases you will also need to use node affinity.\nThe krustlet-wasi and krustlet-wasmcloud ‘virtual nodes’ both have NoExecute and NoSchedule taints with the key kubernetes.io/arch and a provider-defined value (wasm32-wasi or wasm32-wasmcloud respectively). WASM pods must therefore specify a toleration for this taint. For example:\napiVersion:v1kind:Podmetadata:name:hello-wasmspec:containers:- name:hello-wasmimage:webassembly.azurecr.io/hello-wasm:v1tolerations:- effect:NoExecutekey:kubernetes.io/archoperator:Equalvalue:wasm32-wasi # or wasm32-wasmcloud according to module target arch- effect:NoSchedulekey:kubernetes.io/archoperator:Equalvalue:wasm32-wasi # or wasm32-wasmcloud according to module target archIn addition, if the Kubernetes cluster contains ‘standard’ OCI nodes which do not taint themselves, you should prevent Kubernetes from scheduling WASM workloads to those nodes. To do this, you can either taint the OCI nodes (though this may require you to provide suitable tolerations on OCI pods), or you can specify a node selector on the WASM workload to direct it to compatible nodes:\napiVersion:v1kind:Podmetadata:name:hello-wasmspec:# other values as abovenodeSelector:kubernetes.io/arch:wasm32-wasi # or wasm32-wasmcloudIf you get intermittent image pull errors on your WASM workloads, check that they are not inadvertently getting scheduled to OCI nodes.\n","excerpt":"Running Web Assembly (WASM) workloads in Kubernetes The Krustlet …","ref":"/howto/wasm/","title":""},{"body":"Introduction New to Krustlet? Start here:\n What is Krustlet? Quick Start Install Krustlet Writing your first app, part 1 Writing your first app, part 2 Writing your first app, part 3 What to read next  ","excerpt":"Introduction New to Krustlet? Start here:\n What is Krustlet? Quick …","ref":"/intro/","title":""},{"body":"Install Krustlet This guide shows how to install Krustlet.\nFrom the Binary Releases Every release of Krustlet provides compiled releases for a variety of Operating Systems. These compiled releases can be manually downloaded and installed. Please note these instructions will work on Linux, MacOS, and Windows (in PowerShell)\n Download your desired version from the releases page Unpack it (tar -xzf krustlet-v0.6.0-linux-amd64.tar.gz) Find the desired Krustlet provider in the unpacked directory, and move it to its desired destination somewhere in your $PATH (e.g. mv krustlet-wasi /usr/local/bin/ on unix-like systems or mv krustlet-wasi.exe C:\\Windows\\system32\\ on Windows)  From there, you should be able to run the client in your terminal emulator. If your terminal cannot find Krustlet, check to make sure that your $PATH environment variable is set correctly.\nValidating If you’d like to validate the download, checksums can be downloaded from https://krustlet.blob.core.windows.net/releases/checksums-v0.6.0.txt\nWindows As of Krustlet 0.4, there are now Windows builds available. However, there are some caveats. The underlying dependencies used to support Windows do not support certs with IP SANs (subject alternate names). Because of this, the serving certs requested during bootstrap will not work for local development options like minikube or KinD as they do not have an FQDN. So these builds can only be used in environments with an actual hostname/FQDN accessible to the Kubernetes cluster.\nFrom Canary Builds “Canary” builds are versions of Krustlet that are built from master. They are not official releases, and may not be stable. However, they offer the opportunity to test the cutting edge features before they are released.\nHere are links to the common builds:\n checksum file 64-bit Linux (AMD architecture) 64-bit Linux (ARM architecture) 64-bit macOS (AMD architecture) 64-bit Windows  Compiling from Source If you want to compile Krustlet from source, you will need to follow the developer guide.\nNext Steps After installing Krustlet, if you’d like to get started and see something running, go checkout any one of the demos. Each of them has a prebuilt WebAssembly module stored in a registry and a Kubernetes manifest that you can kubectl apply.\nIf you’d like to learn how to write your own simple module in Rust and deploy it, follow through the tutorial to deploy your first application.\n","excerpt":"Install Krustlet This guide shows how to install Krustlet.\nFrom the …","ref":"/intro/install/","title":""},{"body":"Introduction Krustlet is a tool to run WebAssembly workloads natively on Kubernetes. Krustlet acts like a node in your Kubernetes cluster. When a user schedules a Pod with certain node tolerations, the Kubernetes API will schedule that workload to a Krustlet node, which will then fetch the module and run it.\nKrustlet implements the kubelet API, and it will respond to common API requests like kubectl logs or kubectl delete.\nIf you’d like to learn how to deploy Krustlet on your own cluster (or if you’re just getting started), follow the quickstart guide for instructions on deploying your first Kubernetes cluster.\nIn order for your application to run on a Kruslet node, the application must be compiled to WebAssembly and pushed to a container registry. If you’d like to learn more about how to write your own WebAssembly module in Rust and deploy it, follow through the tutorial to deploy your first application.\n","excerpt":"Introduction Krustlet is a tool to run WebAssembly workloads natively …","ref":"/intro/intro/","title":""},{"body":"Quick Start Get started with Krustlet in three easy steps:\n Boot a Kubernetes cluster Boot a Krustlet node Deploy your first application  As Krustlet is under active development, this guide will help you set up a cluster suitable for evaluation, development, and testing purposes.\nStep 1: Boot a Kubernetes Cluster There are many ways to boot up a Kubernetes cluster. You may choose to get up and running in cloud environments or locally on your laptop.\nIf you have already created a Kubernetes cluster, proceed to the next step to install Krustlet on your own Kubernetes cluster.\nFor production use:\n Azure Amazon Elastic Kubernetes Service (EKS)  For development and evaluation purposes, it may make sense to use a VM-based Kubernetes cluster for quick and easy setup and teardown:\n Kubernetes-in-Docker (KinD) Minikube  Step 2: Boot a Krustlet Node Depending on whatever provider you chose in step 1, you now have a few options to boot and register Krustlet with your Kubernetes cluster.\nIf you have your own Kubernetes cluster, you may want to follow the steps in the cloud-based option guides to determine how to set up Krustlet for your own infrastructure.\nFor production use, you’ll want to boot Krustlet on a device that can start a web server on an IP accessible from the Kubernetes control plane.\n Amazon Elastic Kubernetes Service (EKS)  For testing/development environments:\n Kubernetes-in-Docker (KinD) MicroK8s Minikube Windows Subsystem for Linux (WSL2)  Step 3: Deploy your First Application If you just want to get started and see something running, go checkout any one of the demos. Each of them has a prebuilt WebAssembly module stored in a registry and a Kubernetes manifest that you can kubectl apply.\nIf you’d like to learn how to write your own simple module in Rust and deploy it, follow through the tutorial to deploy your first application.\n","excerpt":"Quick Start Get started with Krustlet in three easy steps:\n Boot a …","ref":"/intro/quickstart/","title":""},{"body":"What to read next So you’ve read all the introductory material and have decided you’d like to keep using Krustlet to run your WebAssembly applications. We’ve only just scratched the surface.\nSo what’s next?\nWell, we’ve always been big fans of learning by doing. At this point you should know enough to start a project of your own and start fooling around. As you need to learn new tricks, come back to the documentation.\nWe’ve put a lot of effort into making Krustlet’s documentation useful, easy to read and as complete as possible. The rest of this document explains more about how the documentation works so that you can get the most out of it.\n(Yes, this is documentation about documentation. Rest assured we have no plans to write a document about how to read the document about documentation.)\nHow the documentation is organized Krustlet’s main documentation is broken up into “chunks” designed to fill different needs:\nThe [introductory material][intro] is designed for people new to Krustlet – or to WebAssembly on Kubernetes in general. It doesn’t cover anything in depth, but instead gives a high-level overview of how running apps on Kubernetes“feels”.\nThe [topic guides][topics], on the other hand, dive deep into individual parts of Krustlet. There are complete guides to the internals of Krustlet’s architecture. This is probably where you’ll want to spend most of your time; if you work your way through these guides you should come out knowing pretty much everything there is to know about Krustlet.\n[intro]: README.md [topics]: ../topics/README.md\n","excerpt":"What to read next So you’ve read all the introductory material and …","ref":"/intro/readnext/","title":""},{"body":"Writing your first app, part 1 Let’s learn by example.\nThroughout this tutorial, we’ll walk you through the creation of a basic WASI application. Once ready, we will package that application and install it onto the Kubernetes cluster using krustlet.\nThe tutorial will consist of three parts:\n Building the application Publishing the application to a registry Running the application with Krustlet  Prerequisites We’ll assume you have Cargo (a package management system for Rust) installed already.\nIf you’re compiling the application written in C, you’ll want to install the WASI SDK, though if you’re following the tutorial with the Rust example, this step is optional.\nIn part 2 of this tutorial, we will be publishing our application to a registry hosted on Microsoft Azure. The steps assume you have an Azure account and the az CLI installed. However, there are other cloud providers available with their own solutions, and if you’re feeling particularly brave, you can run your own registry on your own infrastructure. You’ll also need wasm-to-oci (a tool for publishing WebAssembly modules to a registry).\nWe’ll assume you have Krustlet installed already. See the quickstart guide for advice on how to boot a Kubernetes cluster and install Krustlet.\nIf you’re having trouble going through this tutorial, please post an issue to deislabs/krustlet to chat with other Krustlet users who might be able to help.\nCreating your first application For this tutorial, we’ll be creating an example application written either in C or in Rust.\nThe application a very simple “hello world” application, running forever and printing “hello world!” every 5 seconds to standard output.\nOption 1: From C First, let’s write the application in C. To create your app, type this command:\n$ mkdir demo $ cd demo $ touch main.c The C code here uses standard POSIX APIs, and doesn’t have any knowledge of WASI internals.\n#include \u003cstdio.h\u003e#include \u003cunistd.h\u003e int main() { while(1) { printf(\"Hello, World!\\n\"); sleep(5); } return 0; } The wasi-sdk provides a clang which is configured to target WASI. We can compile our program like so:\n$ clang main.c -o demo.wasm This is just regular clang, configured to use a WebAssembly target and sysroot. The output of clang here is a standard WebAssembly module:\n$ file demo.wasm demo.wasm: WebAssembly (wasm) binary module version 0x1 (MVP) Option 2: From Rust The same application can be written in Rust. First, go ahead and start a new project:\n$ cargo new --bin demo Now, let’s port the C program defined earlier to Rust. In src/main.rs:\nusestd::time::Duration;usestd::thread::sleep;fn main(){loop{println!(\"Hello, World!\");sleep(Duration::from_secs(5));}}In order to build it, we first need to install a WASI-enabled Rust toolchain:\n$ rustup target add wasm32-wasi $ cargo build --release --target wasm32-wasi We should now have the WebAssembly module created in target/wasm32-wasi/release:\n$ file target/wasm32-wasi/release/demo.wasm demo.wasm: WebAssembly (wasm) binary module version 0x1 (MVP) Optional: executing with wasmtime The WebAssembly module demo.wasm we just compiled either from C or Rust is simply a single file containing a self-contained WASM module.\nwasmtime is a standalone JIT-style runtime for WebAssembly and WASI. It runs WebAssembly code outside of the web, and can be used both as a command-line utility or as a library embedded in a larger application.\nWe can execute our application with wasmtime directly, like so:\n$ wasmtime demo.wasm Hello, World! Hello, World! Hello, World! ^C To exit the program, enter CTRL+C with your keyboard.\nGreat! Our program runs as expected!\nWhen you’re comfortable with the application, read part 2 of this tutorial to learn about publishing our application to a registry, where Krustlet will be able to find it and run it.\n","excerpt":"Writing your first app, part 1 Let’s learn by example.\nThroughout this …","ref":"/intro/tutorial01/","title":""},{"body":"Writing your first app, part 2 This tutorial begins where Tutorial 1 left off. We’ll walk through the process to set up your personal registry and publish your application to that registry.\nFor this tutorial, we will be creating a registry hosted on Microsoft Azure, but there are other cloud providers that provide their own solutions, and you can run one on your own infrastructure, too!\nWhat is a registry, and what is wasm-to-oci? A registry allows you to store your local WebAssembly modules in the cloud. With a registry, you can backup your personal modules, share your projects, and collaborate with others.\nwasm-to-oci is an open source project that understands how to communicate with a registry. It takes a module you’ve built locally on your computer and publishes it to the registry, making it publicly available for others to access.\nCreate a registry This tutorial uses the Azure CLI to create an Azure Container Registry. We will be using this registry to publish our modules and provide Krustlet the URL for fetching these modules.\nThe steps here assume you have an Azure account and the az CLI installed. However, there are other cloud providers available with their own solutions, and if you’re feeling particularly brave, you can run your own registry on your own infrastructure.\nCreate a resource group An Azure resource group is a logical container into which Azure resources are deployed and managed.\nThe following example creates a resource group named myResourceGroup in the eastus region. You may want to change it to a region closer to you. You can find out what regions are available with az account list-locations, and you can set your default region with az configure --defaults location=\u003clocation\u003e.\nCreate a resource group with the az group create command.\n$ az group create --name myResourceGroup --location eastus Create a container registry In this tutorial, we will be creating a basic registry, which is cost-optimized for developers learning about Azure Container Registry. For details on available service tiers, see Container registry SKUs in the Azure documentation.\nCreate an ACR instance using the az acr create command. The registry name must be unique within Azure, and contain 5-50 alphanumeric characters.\nIn the following example, mycontainerregistry007 is used as the name. Update this to a unique value.\n$ az acr create --sku Basic --resource-group myResourceGroup --name mycontainerregistry007 When the registry is created, the output is similar to the following:\n{ \"adminUserEnabled\": false, \"creationDate\": \"2019-01-08T22:32:13.175925+00:00\", \"id\": \"/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/myResourceGroup/providers/Microsoft.ContainerRegistry/registries/mycontainerregistry007\", \"location\": \"eastus\", \"loginServer\": \"mycontainerregistry007.azurecr.io\", \"name\": \"mycontainerregistry007\", \"provisioningState\": \"Succeeded\", \"resourceGroup\": \"myResourceGroup\", \"sku\": { \"name\": \"Basic\", \"tier\": \"Basic\" }, \"status\": null, \"storageAccount\": null, \"tags\": {}, \"type\": \"Microsoft.ContainerRegistry/registries\" } Take note of the loginServer field. That is the URL for our registry. We’ll need to know that when we publish our application in a bit.\nLog in Now that our registry was created, we can go ahead and authenticate with this registry to publish our application:\n$ az acr login --name mycontainerregistry007 Publish your app Now that we’ve created our registry and are logged in, we can publish our application using wasm-to-oci.\nwasm-to-oci is a tool for publishing WebAssembly modules to a registry. It packages the module and uploads it to the registry. Krustlet understands the registry API and will fetch the module based on the URL you uploaded it to.\nTo publish our application, we need to come up with a name and a version number. Our loginServer field from earlier was mycontainerregistry007.azurecr.io, and we want to name our application krustlet-tutorial, version v1.0.0.\nThe pattern for a registry URL is:\n\u003cURL\u003e/\u003cNAME\u003e:\u003cVERSION\u003e In our case, our application URL will look like mycontainerregistry007.azurecr.io/krustlet-tutorial:v1.0.0. Great!\nLet’s publish that now:\n$ wasm-to-oci push demo.wasm mycontainerregistry007.azurecr.io/krustlet-tutorial:v1.0.0 demo.wasm is the filename of the WebAssembly module we compiled during part 1 of this tutorial. If you are publishing the Rust example, use target/wasm32-wasi/debug/demo.wasm instead.\nCreate a container registry pull secret Unless your container registry is enabled with anonymous access, you need to authenticate krustlet to pull images from it. At the moment, there is no flag in the Azure portal to make a registry public, but you can create a support ticket to have it enabled manually.\nWithout public access to the container registry, you need to create a Kubernetes pull secret. The steps below for Azure are extracted from the Azure documentation, and repeated here for convenience.\nCreate a service principal and assign a role in Azure Below is a bash script that will create a service principal for pulling images for the registry. Replace \u003ccontainer-registry-name\u003e with mycontainerregistry007.\n#!/bin/bash  # Modify for your environment. # ACR_NAME: The name of your Azure Container Registry # SERVICE_PRINCIPAL_NAME: Must be unique within your AD tenant ACR_NAME=\u003ccontainer-registry-name\u003e SERVICE_PRINCIPAL_NAME=acr-service-principal # Obtain the full registry ID for subsequent command args ACR_REGISTRY_ID=$(az acr show --name $ACR_NAME --query id --output tsv) # Create the service principal with rights scoped to the registry. # Default permissions are for docker pull access. Modify the '--role' # argument value as desired: # acrpull: pull only # acrpush: push and pull # owner: push, pull, and assign roles SP_PASSWD=$(az ad sp create-for-rbac --name http://$SERVICE_PRINCIPAL_NAME --scopes $ACR_REGISTRY_ID --role acrpull --query password --output tsv) SP_APP_ID=$(az ad sp show --id http://$SERVICE_PRINCIPAL_NAME --query appId --output tsv) # Output the service principal's credentials; use these in your services and # applications to authenticate to the container registry. echo \"Service principal ID: $SP_APP_ID\" echo \"Service principal password: $SP_PASSWD\" If you do not want to create a service principal in Azure, you can also use the registry Admin username and password which gives full access to the registry and is not generally recommended. This is not enabled by default. Go to the Azure portal and the settings for your registry and the Access keys menu. There you can enable Admin access and use the associated username instead of the service principal ID and the password when creating the pull secret below.\nUse the service principal Create an image pull secret in Kubernetes:\nkubectl create secret docker-registry \u003cacr-secret-name\u003e \\ --namespace \u003cnamespace\u003e \\ --docker-server=mycontainerregistry007.azurecr.io \\ --docker-username=\u003cservice-principal-ID\u003e \\ --docker-password=\u003cservice-principal-password\u003e where \u003cacr-secret-name\u003e is a name you give this secret, \u003cservice-principal-ID\u003e and \u003cservice-principal-password\u003e are taken from the output of the bash script above. The --namespace can be omitted if you are using the default Kubernetes namespace.\nNext steps When you’re comfortable with publishing your application with wasm-to-oci, read part 3 of this tutorial to install your application.\n","excerpt":"Writing your first app, part 2 This tutorial begins where Tutorial 1 …","ref":"/intro/tutorial02/","title":""},{"body":"Writing your first app, part 3 This tutorial begins where Tutorial 2 left off. We’ll walk through the process for installing your first application written in WebAssembly into your Kubernetes cluster, then test our application using kubectl.\nScheduling pods on the Krustlet In Kubernetes, Pods are the smallest deployable units of compute that can be created and managed in Kubernetes. In other words, your application runs inside a Pod, and we can inspect the status of the application by inspecting the Pod.\nKrustlet listens for pods requesting a node with the wasm32-wasi architecture. To schedule a Pod that Krustlet understands, we need to provide Kubernetes with a YAML file describing our Pod.\nCreate a new file and call it krustlet-tutorial.yaml:\napiVersion:v1kind:Podmetadata:name:krustlet-tutorialspec:containers:- name:krustlet-tutorialimage:mycontainerregistry007.azurecr.io/krustlet-tutorial:v1.0.0imagePullSecrets:- name:\u003cacr-secret\u003etolerations:- key:\"kubernetes.io/arch\"operator:\"Equal\"value:\"wasm32-wasi\"effect:\"NoExecute\"- key:\"kubernetes.io/arch\"operator:\"Equal\"value:\"wasm32-wasi\"effect:\"NoSchedule\"Let’s break this file down:\n apiVersion: which version of the Kubernetes API are we targeting? kind: what type of workload are we deploying? metadata.name: what is the name of our workload? spec.containers[0].name: what should I name this module? spec.containers[0].image: where can I find the module? spec.imagePullSecrets[0].name: which name has the image pull secret? spec.tolerations: what kind of node am I allowed to run on?  To deploy this workload to Kubernetes, we use kubectl.\n$ kubectl create -f krustlet-tutorial.yaml Now that the workload has been scheduled, Krustlet should start spewing out some logs in its terminal window, reporting updates on the workload that was scheduled.\nWe can check the status of our pod:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE krustlet-tutorial 1/1 Running 0 18s We can also inspect the logs, too:\n$ kubectl logs krustlet-tutorial Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Cleanup Once you’re finished with this tutorial, you can destroy the cluster and the registry.\nDestroying the cluster can be accomplished with:\n$ kind delete cluster And destroying the registry can be accomplished by removing the resource group.\n$ az group delete --name myResourceGroup Conclusion This concludes the basic tutorial. Congratulations!\nIf you are familiar with Krustlet and are interested in more in-depth topics, check out the Topic Guides.\nYou might also be scratching your head on what to read next.\n","excerpt":"Writing your first app, part 3 This tutorial begins where Tutorial 2 …","ref":"/intro/tutorial03/","title":""},{"body":"Topic Guides Introductions to all the key components of Krustlet in more detail:\n Krustlet architecture Providers Configuration The Container Storage Interface Glossary Plugin system  ","excerpt":"Topic Guides Introductions to all the key components of Krustlet in …","ref":"/topics/","title":""},{"body":"Krustlet architecture This document describes the Krustlet architecture at a high level.\nThe purpose of Krustlet Krustlet acts as a Kubernetes Kubelet by listening on the Kubernetes API’s event stream for new Pod requests that match a particular set of node selectors, scheduling those workloads to run using a WASI-based runtime instead of a container-based runtime.\nImplementation Krustlet is written in Rust.\nBy acting as a Kubelet, Krustlet uses Kubernetes client libraries to communicate with the Kubernetes API. Currently, the client libraries use HTTP(s) as the communication protocol between Krustlet and the Kubernetes API, using JSON as the data format for serializing and de-serializing request bodies.\nKrustlet sends status updates about scheduled pods to the Kubernetes API. Therefore, it does not require its own database.\nProviders Krustlet uses providers to interact with a given runtime. The kubelet crate has the common functionality for listing to a stream of pods (an “Informer” in Kubernetes parlance), keeping a node status updated, and handling requests from the Kubernetes API. The Kubelet expects to be given a provider (something that implements the Provider interface) which knows how to create “containers” for the runtime it is implementing. Note that these might not actually be Docker containers (they definitely are not if you are using one of the providers implemented in this repository), but Kubernetes still thinks of them as a “container.” The basic workflow is like this:\n kubelet receives a pod event from the stream Depending on the event type (added, modified, deleted), it calls the corresponding Provider method and creates, updates, or stops/deletes the “container” The Provider does work and returns an error if there is a problem  ","excerpt":"Krustlet architecture This document describes the Krustlet …","ref":"/topics/architecture/","title":""},{"body":"Configuration The kubelet crate supports configuration via the command line, configuration file or environment variables.\nNOTE: Custom kubelets built using the kubelet crate can choose which of these methods to support, or may choose to bypass kubelet’s built-in configuration system in favour of their own. krustlet-wasi uses standard configuration and supports all configuration methods.\nNOTE: Certain flags must be handled at the provider or custom kubelet level. If you are building a custom kubelet using the kubelet crate, please see the “Notes to kubelet implementers” section below.\nConfiguration values    Command line Environment variable Configuration file Description     -a, –addr KRUSTLET_ADDRESS listenerAddress The address on which the kubelet should listen   –data-dir KRUSTLET_DATA_DIR dataDir The path under which the kubelet should store data (e.g. logs, container images, etc.). The default is $HOME/.krustlet   –hostname KRUSTLET_HOSTNAME hostname The name of the host where the kubelet runs. Defaults to the hostname of the machine where the kubelet is running; pass this if the name in the TLS certificate does not match the actual machine name   –max-pods MAX_PODS maxPods The maximum number of pods to schedule on the kubelet at any one time. The default is 110   -n, –node-ip KRUSTLET_NODE_IP nodeIP The IP address of the node registered with the Kubernetes master. Defaults to the IP address of the kubelet hostname, as obtained from DNS   –node-labels NODE_LABELS nodeLabels The labels to apply to the node when it registers in the cluster. See below for format   –node-name KRUSTLET_NODE_NAME nodeName The name by which to refer to the kubelet node in Kubernetes. Defaults to the hostname   -p, –port KRUSTLET_PORT listenerPort The port on which the kubelet should listen. The default is 3000   –cert-file KRUSTLET_CERT_FILE tlsCertificateFile The path to the TLS certificate for the kubelet. The default is (data directory)/config/krustlet.crt   –private-key-file KRUSTLET_PRIVATE_KEY_FILE tlsPrivateKeyFile The path to the private key for the TLS certificate. The default is (data directory)/config/krustlet.key   –insecure-registries KRUSTLET_INSECURE_REGISTRIES insecureRegistries A list of registries that should be accessed using HTTP instead of HTTPS. On the command line or environment variable, use commas to separate multiple registries   –x-allow-local-modules KRUSTLET_ALLOW_LOCAL_MODULES allowLocalModules If true, the kubelet should recognise references prefixed with ‘fs’ as indicating a filesystem path rather than a registry location. This is an experimental flag for use in development scenarios where you don’t want to repeatedly push your local builds to a registry; it is likely to be removed in a future version when we have a more comprehensive toolchain for local development.    Node labels format If you specify node labels on the command line or in an environment variable, the format is a comma-separated list of name=value pairs. For example:\n--node-labels mylabel=foo,myotherlabel=bar If you specify node labels in the configuration file, the format is key-value pairs. For example:\n{ \"node_labels\": { \"mylabel\": \"foo\", \"myotherlabel\": \"bar\" } } Configuration file location By default, the configuration file is located at $HOME/.krustlet/config/config.json. The kubelet crate does not define a common way to override this. However, custom kubelets built on kubelet may provide such a mechanism.\nThe krustlet-wasi kubelet does not currently provide a way to override the default location.\nTODO: should we build in a standard way of overriding the file location?\nPrecedence If you specify the same setting in multiple places - for example, both in the configuration file and on the command line - then the precedence is:\n Command line flags take precedence over environment variables Environment variables take precedence over the configuration file  This allows you to conveniently override individual settings from a configuration file, for example by writing MAX_PODS=200 krustlet-wasi or krustlet-wasi --max-pods 200.\nIf you specify node labels in multiple places, the collections are not combined: the place with the highest precedence takes effect and all others are ignored.\nNotes to kubelet implementers Some flags require you to support them in your provider or main code - they are not implemented automatically by the kubelet core. These flags are as follows:\n --bootstrap-file - should be passed to kubelet::bootstrap if you use the bootstrapping feature --data-dir - this should be used to construct the FileStore if you use one --x-allow-local-modules - if specified you should compose a FileSystemStore onto your normal store  See the krustlet-wasi.rs file for examples of how to honour these flags.\nIf you can’t honour a flag value in your particular scenario, then you should still check for it and return an error, rather than silently ignoring it.\n","excerpt":"Configuration The kubelet crate supports configuration via the command …","ref":"/topics/configuration/","title":""},{"body":"The Container Storage Interface The Container Storage Interface (CSI) is a standardized plugin system that enables many different types of storage systems to\n Automatically provision storage volumes as needed Mount volumes to pods as needed Unmount volumes from deleted or removed pods, and Destroy storage volumes after they’ve been de-commissioned.  Krustlet introduced this feature in v0.6.0 and is currently in alpha status. Many features that Kubernetes supports such as “Block” mounting or read-only access modes are currently unavailable, but will become available as the feature stabilizes.\nWhy CSI? Without CSI support, adding a new storage system to a Provider requires checking code into the core Krustlet repository, and it requires each Provider to call this code in order to support the new volume type. Any changes to the storage system won’t become available until the next Krustlet release, and could be painful for many Providers to adopt these new changes.\nCSI addresses these issues by enabling storage plugins to be developed out-of-tree, deployed alongside a Krustlet Provider, and consumed through standard Kubernetes storage primitives: PersistentVolumeClaims (PVC), PersistentVolumes (PV), and StorageClasses (SC).\nHow do I deploy a CSI driver alongside a Krustlet Provider? Please see the HOWTO guide for more information.\nHow do I use a CSI Volume? Please see the HOWTO guide for more information.\nWhere can I find CSI Drivers? CSI drivers are maintained and distributed by the community. You can find example CSI drivers in the kubernetes-csi organization on GitHub. These are provided purely for illustrative purposes, and are not intended for use in production.\n","excerpt":"The Container Storage Interface The Container Storage Interface (CSI) …","ref":"/topics/csi/","title":""},{"body":"Glossary Here is where you will find definitions for commmon terminology used across Krustlet.\nKubelet The kubelet is a key piece of the Kubernetes architecture. A kubelet is a “node agent” that runs on each node in a Kubernetes cluster. It registers as a node with the Kubernetes API, waiting for new pods provided by the API, and ensures the workloads in those pods are running and healthy.\nPod A pod is the simplest execution unit in Kubernetes that you can create and destroy using the Kubernetes API. Nearly every workload type available in Kubernetes (Deployments, StatfulSets, DaemonSets, Jobs, etc.) uses pods as the basic unit of work. In other words, a pod represents a single unit of work in your cluster.\nProvider A provider is an abstract interface within Krustlet. Providers describe the verbs and actions a WebAssembly runtime (like wasmtime) must provide in order for that runtime to work as a kubelet.\nThe primary responsibility of a provider is to execute a workload (or schedule it on an external executor), monitor that workload, and expose important details back to Kubernetes using the Kubelet API.\nSee also the topic guide on providers.\n","excerpt":"Glossary Here is where you will find definitions for commmon …","ref":"/topics/glossary/","title":""},{"body":"Plugin System Overview Krustlet partially implements the plugin discovery system used by the mainline Kubelet for purposes of supporting CSI. The CSI documentation points at the device plugin documentation, but upon further investigation/reverse engineering, we determined that CSI plugins use the auto plugin discovery method implemented here. You can also see other evidence of this in the csi-common code and the Node Driver Registrar documentation.\nWhat is not supported? Currently we do not support the DevicePlugin type or the aforementioned newer device plugin system. Currently we do not have plans to implement it, but that could change in the future as needs/uses evolve\nHow does it work? The plugin registration system has an event driven loop for discovering and registering plugins:\n Kubelet using a file system watcher to watch the given directory Plugins wishing to register themselves with Kubelet must open a Unix domain socket (henceforth referred to as just “socket”) in the watched directory When Kubelet detects a new socket, it connects to the discovered socket and attempts to do a GetInfo gRPC call. Using the info returned from the GetInfo call, Kubelet performs validation to make sure it supports the correct version of the API requested by the plugin and that the plugin is not already registered. If it is a CSIPlugin type, the info will also contain another path to a socket where the CSI driver is listening If validation succeeds, Kubelet makes a NotifyRegistrationStatus gRPC call on the originally discovered socket to inform the plugin that it has successfully registered  Additional information In normal Kubernetes land, most CSI plugins register themselves with the Kubelet using the Node Driver Registrar sidecar container that runs with the actual CSI driver. It has the responsibilty for creating the socket that Kubelet discovers.\n","excerpt":"Plugin System Overview Krustlet partially implements the plugin …","ref":"/topics/plugin_system/","title":""},{"body":"Providers The default runtime for the Krustlet project is wasi.\nThe wasi runtime uses a project called wasmtime. wasmtime is a standalone JIT-style host runtime for WebAssembly modules. It is focused primarily on standards compliance with the WASM specification as it relates to WASI. If your WebAssembly module complies with the WebAssembly specification, wasmtime can run it.\nIt’s important to note that the WASI standard and wasmtime are still under heavy development. There are some key features (like networking) that are currently missing, but will be made available in future updates.\nAdditional Providers There are various other providers available as well.\n wasmcloud: The wasmcloud runtime is a secure WebAssembly host runtime, connecting “actors” and “capability providers” together to connect your WebAssembly runtime to cloud-native services like message brokers, databases, or other external services normally unavailable to the WebAssembly runtime. This provider used to be available in this repo but was moved under the wasmCloud project so it could be maintained both by the Krustlet maintainers and the wasmCloud maintainers. CRI: A Container Runtime Interface provider implementation for Krustlet. This runtime allows you to run the containers you know and love within Krustlet.  ","excerpt":"Providers The default runtime for the Krustlet project is wasi.\nThe …","ref":"/topics/providers/","title":""},{"body":"","excerpt":"","ref":"/categories/","title":"Categories"},{"body":"","excerpt":"","ref":"/tags/","title":"Tags"}]